{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### library import \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# import all_data\n",
    "all_data_age = pd.read_pickle('all_data_age')\n",
    "all_data_hw = pd.read_pickle('all_data_hw')\n",
    "all_data_icd = pd.read_pickle('all_data_icd')\n",
    "all_data_inj = pd.read_pickle('all_data_inj')\n",
    "all_data_vital = pd.read_pickle('all_data_vital')\n",
    "lab50_corr = pd.read_pickle('lab50_corr')\n",
    "all_data_age = pd.read_pickle('all_data_age')\n",
    "list_sex_opz = pd.read_pickle('list_sex_opz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import opz data\n",
    "df_lab_opz = pd.read_pickle('df_lab_opz.pkl')\n",
    "df_name_opz = pd.read_pickle('df_name_opz.pkl')\n",
    "df_vital_opz = pd.read_pickle('df_vital_opz.pkl')\n",
    "df_hw_opz = pd.read_pickle('df_hw_opz.pkl')\n",
    "df_death_opz = pd.read_pickle('pt_id/df_death_opz.pkl')\n",
    "df_sex_opz = pd.read_pickle('df_sex_opz.pkl')\n",
    "df_age_opz = pd.read_pickle('df_age_opz.pkl')\n",
    "df_inj_opz = pd.read_pickle('pt_id/df_inj_opz.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read PT_ID (patinent ID)\n",
    "list_PT_ID = list(df_inj_opz['PT_ID'])\n",
    "list_PT_ID = list(set(list_PT_ID))\n",
    "list_PT_ID = sorted(list_PT_ID)\n",
    "print(\"#number of patients:\",len(list_PT_ID))\n",
    "\n",
    "#preprocessing vital data\n",
    "def preprocess_df_vital_opz(df_vital_opz):\n",
    "    clean_df_vital_opz = df_vital_opz.drop(['Time', '診療科', '呼吸数'], axis = 1)\n",
    "    clean_df_vital_opz['体温'] = clean_df_vital_opz['体温'].astype(float)\n",
    "    clean_df_vital_opz['脈拍'] = clean_df_vital_opz['脈拍'].astype(float)\n",
    "    clean_df_vital_opz['血圧(最高)'] = clean_df_vital_opz['血圧(最高)'].astype(float)\n",
    "    clean_df_vital_opz['血圧(最低)'] = clean_df_vital_opz['血圧(最低)'].astype(float)\n",
    "\n",
    "    clean_df_vital_opz = clean_df_vital_opz.sort_values(by = 'Date')\n",
    "    \n",
    "    \n",
    "    return clean_df_vital_opz\n",
    "\n",
    "preprocess_df_vital_opz(df_vital_opz)\n",
    "\n",
    "clean_df_vital_opz=preprocess_df_vital_opz(df_vital_opz)\n",
    "clean_df_vital_opz=preprocess_df_vital_opz(df_vital_opz)\n",
    "df_one_vital = clean_df_vital_opz[clean_df_vital_opz[\"PT_ID\"]==\"920076678\"]\n",
    "df_one_vital\n",
    "\n",
    "#患者さん一人のデータでDateごとの中央値を出す\n",
    "df_one_vital_taionn = df_one_vital.groupby('Date').agg({'体温': 'median'})\n",
    "df_one_vital_myaku = df_one_vital.groupby('Date').agg({'脈拍': 'median'})\n",
    "df_one_vital_blood_h = df_one_vital.groupby('Date').agg({'血圧(最高)': 'median'})\n",
    "df_one_vital_blood_l = df_one_vital.groupby('Date').agg({'血圧(最低)': 'median'})\n",
    "#4つをDateをkeyにconcat\n",
    "df_one_vital_median = pd.concat([df_one_vital_taionn, df_one_vital_myaku, df_one_vital_blood_h, df_one_vital_blood_l], axis=1)\n",
    "df_one_vital_median\n",
    "\n",
    "###PT_IDのリストぶんforループ\n",
    "#clean_df_vital_opzの正規化を行う\n",
    "clean_df_vital_opz\n",
    "#MinMax法\n",
    "mm = preprocessing.MinMaxScaler()\n",
    "ttt = clean_df_vital_opz\n",
    "ttt = ttt.values[:, 2:6]\n",
    "ttt_mm = mm.fit_transform(ttt)\n",
    "list_columns = ['体温', '脈拍', '血圧(最高)', '血圧(最低)']\n",
    "ttt_mm = pd.DataFrame(ttt_mm, columns = list_columns)\n",
    "ttt_mm\n",
    "ppp = clean_df_vital_opz.reset_index(drop=True)\n",
    "ppp = ppp.drop(['体温', '脈拍', '血圧(最高)', '血圧(最低)'], axis = 1)\n",
    "pt_mmm = pd.concat([ttt_mm, ppp], axis = 1)\n",
    "clean_df_vital_opz = pt_mmm\n",
    "\n",
    "all_data_vital=[]\n",
    "for x in list_PT_ID:\n",
    "    #患者さん一人ぶん\n",
    "    df_x_vital = clean_df_vital_opz[clean_df_vital_opz[\"PT_ID\"]==str(x)]\n",
    "    #患者さん一人のデータでDateごとの中央値を出す\n",
    "    df_vital_taionn = df_x_vital.groupby('Date').agg({'体温': 'median'})\n",
    "    df_vital_myaku = df_x_vital.groupby('Date').agg({'脈拍': 'median'})\n",
    "    df_vital_blood_h = df_x_vital.groupby('Date').agg({'血圧(最高)': 'median'})\n",
    "    df_vital_blood_l = df_x_vital.groupby('Date').agg({'血圧(最低)': 'median'})\n",
    "\n",
    "    #4つをDateをkeyにconcat\n",
    "    df_vital_median = pd.concat([df_vital_taionn, df_vital_myaku, df_vital_blood_h, df_vital_blood_l], axis=1)\n",
    "    #print(df_vital_median)\n",
    "    \n",
    "    all_data_vital.append(df_vital_median)\n",
    "\n",
    "all_data_vital[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#injきれいにしたい\n",
    "def preprocess_df_inj_opz(df_inj_opz):\n",
    "    #injきれいにしたい\n",
    "    clean_df_inj_opz = df_inj_opz.drop(['入力日', '薬剤名', '薬剤コード', '診療科コード', '診療科', '入力時刻', '更新時刻','更新日','キャンセル日','キャンセル時刻','キャンセル種別','指示用量','指示単位','成分単位量','成分単位','手技名','用法内容','実施状況','入外区分'], axis = 1)\n",
    "\n",
    "    clean_df_inj_opz['薬価基準収載医薬品コード'].value_counts()\n",
    "    #このうち抗がん剤のみ抽出する\n",
    "    clean_df_inj_opz_423 = clean_df_inj_opz[df_inj_opz['薬価基準収載医薬品コード'].str.contains('423', na = False)]\n",
    "    clean_df_inj_opz_424 = clean_df_inj_opz[df_inj_opz['薬価基準収載医薬品コード'].str.contains('424', na = False)]\n",
    "    clean_df_inj_opz_429 = clean_df_inj_opz[df_inj_opz['薬価基準収載医薬品コード'].str.contains('429', na = False)]\n",
    "    clean_df_inj_opz = pd.concat([clean_df_inj_opz_423, clean_df_inj_opz_424, clean_df_inj_opz_429])\n",
    "    \n",
    "    ###########頭文字でないの9個まぎれていたので消したい\n",
    "    ##'3999424G1025','6343424D3019','1242405A1038','1242401A1315','6343424D2012','1242406G1035','6363424D3035', '3999423A1125','3999423A1044'\n",
    "    drop_index_inj3999424G1025 = clean_df_inj_opz.index[clean_df_inj_opz['薬価基準収載医薬品コード'] == '3999424G1025']\n",
    "    clean_df_inj_opz = clean_df_inj_opz.drop(drop_index_inj3999424G1025)\n",
    "\n",
    "    drop_index_inj6343424D3019 = clean_df_inj_opz.index[clean_df_inj_opz['薬価基準収載医薬品コード'] == '6343424D3019']\n",
    "    clean_df_inj_opz = clean_df_inj_opz.drop(drop_index_inj6343424D3019)\n",
    "\n",
    "    drop_index_inj1242405A1038 = clean_df_inj_opz.index[clean_df_inj_opz['薬価基準収載医薬品コード'] == '1242405A1038']\n",
    "    clean_df_inj_opz = clean_df_inj_opz.drop(drop_index_inj1242405A1038)\n",
    "\n",
    "    drop_index_inj1242401A1315 = clean_df_inj_opz.index[clean_df_inj_opz['薬価基準収載医薬品コード'] == '1242401A1315']\n",
    "    clean_df_inj_opz = clean_df_inj_opz.drop(drop_index_inj1242401A1315)\n",
    "\n",
    "    drop_index_inj6343424D2012 = clean_df_inj_opz.index[clean_df_inj_opz['薬価基準収載医薬品コード'] == '6343424D2012']\n",
    "    clean_df_inj_opz = clean_df_inj_opz.drop(drop_index_inj6343424D2012)\n",
    "\n",
    "    drop_index_inj1242406G1035 = clean_df_inj_opz.index[clean_df_inj_opz['薬価基準収載医薬品コード'] == '1242406G1035']\n",
    "    clean_df_inj_opz = clean_df_inj_opz.drop(drop_index_inj1242406G1035)\n",
    "\n",
    "    drop_index_inj6363424D3035 = clean_df_inj_opz.index[clean_df_inj_opz['薬価基準収載医薬品コード'] == '6363424D3035']\n",
    "    clean_df_inj_opz = clean_df_inj_opz.drop(drop_index_inj6363424D3035)\n",
    "\n",
    "    drop_index_inj3999423A1125 = clean_df_inj_opz.index[clean_df_inj_opz['薬価基準収載医薬品コード'] == '3999423A1125']\n",
    "    clean_df_inj_opz = clean_df_inj_opz.drop(drop_index_inj3999423A1125)\n",
    "\n",
    "    drop_index_inj3999423A1044 = clean_df_inj_opz.index[clean_df_inj_opz['薬価基準収載医薬品コード'] == '3999423A1044']\n",
    "    clean_df_inj_opz = clean_df_inj_opz.drop(drop_index_inj3999423A1044)\n",
    "    \n",
    "    ###########\n",
    "    clean_df_inj_opz['有＝１'] = 1\n",
    "    \n",
    "    \n",
    "\n",
    "    return clean_df_inj_opz\n",
    "\n",
    "clean_df_inj_opz=preprocess_df_inj_opz(df_inj_opz)\n",
    "###PT_IDのリストぶんforループ\n",
    "all_data_inj=[]\n",
    "for x in list_PT_ID:\n",
    "    #患者一人ぶん\n",
    "    df_one_inj = clean_df_inj_opz[clean_df_inj_opz[\"PT_ID\"]==str(x)]\n",
    "\n",
    "    #dateをリストに変換\n",
    "    #そして重複を削除\n",
    "    #並び替え\n",
    "    list_one_inj_date = list(set(df_one_inj['Date']))\n",
    "    list_one_inj_date.sort()\n",
    "\n",
    "    #list_date_injは仮　本当はlist_one_inj_date\n",
    "    list_date_inj = list_one_inj_date #['2017-07-03','2017-06-05']\n",
    "    list_one_inj = []\n",
    "    for x in list_date_inj:\n",
    "        #Date=xの時のデータ\n",
    "        df_one_inj_x = df_one_inj[df_one_inj['Date'] == str(x)]\n",
    "        df_one_inj_x = df_one_inj_x.drop(['PT_ID', 'Date'], axis = 1)\n",
    "        df_one_inj_x = df_one_inj_x.set_index(\"薬価基準収載医薬品コード\")\n",
    "        ## 注射二回打っている？場合の処理\n",
    "        grouped = df_one_inj_x.groupby(level=0)  \n",
    "        df_one_inj_x = grouped.last() \n",
    "\n",
    "        #date×検査項目にする\n",
    "        df_one_inj_x=df_one_inj_x.T\n",
    "        df_one_inj_x = df_one_inj_x.rename(index = {'有＝１': str(x)})\n",
    "        list_one_inj.append(df_one_inj_x)\n",
    "    merged_df_one_inj=pd.concat(list_one_inj)\n",
    "    all_data_inj.append(merged_df_one_inj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##薬価基準収載医薬品コードはじめの7桁が主成分なので最初の7桁でグループ分け？する\n",
    "clean_df_inj_opz['薬価基準収載医薬品コード最初７桁'] = clean_df_inj_opz['薬価基準収載医薬品コード'].str[:7]\n",
    "list_namecode = [clean_df_inj_opz['薬価基準収載医薬品コード最初７桁']]\n",
    "for x in list_namecode:\n",
    "    list_namecode_ = 'inj'+ x\n",
    " \n",
    "    clean_df_inj_opz = clean_df_inj_opz.drop(['薬価基準収載医薬品コード最初７桁'], axis = 1)\n",
    "    clean_df_inj_opz['薬価基準収載医薬品コード最初７桁'] = list_namecode_\n",
    "clean_df_inj_opz\n",
    "\n",
    "####PT_IDのリストぶんforループ\n",
    "all_data_inj=[]\n",
    "for x in list_PT_ID:\n",
    "    #患者一人ぶん\n",
    "    df_one_inj = clean_df_inj_opz[clean_df_inj_opz[\"PT_ID\"]==str(x)]\n",
    "\n",
    "    #dateをリストに変換\n",
    "    #そして重複を削除\n",
    "    #並び替え\n",
    "    list_one_inj_date = list(set(df_one_inj['Date']))\n",
    "    list_one_inj_date.sort()\n",
    "\n",
    "    #list_date_injは仮　本当はlist_one_inj_date\n",
    "    list_date_inj = list_one_inj_date #['2017-07-03','2017-06-05']\n",
    "    list_one_inj = []\n",
    "    for x in list_date_inj:\n",
    "        #Date=xの時のデータ\n",
    "        df_one_inj_x = df_one_inj[df_one_inj['Date'] == str(x)]\n",
    "        df_one_inj_x = df_one_inj_x.drop(['PT_ID', 'Date', '薬価基準収載医薬品コード'], axis = 1)\n",
    "        df_one_inj_x = df_one_inj_x.set_index(\"薬価基準収載医薬品コード最初７桁\")\n",
    "        ## 注射二回打っている？場合の処理\n",
    "        grouped = df_one_inj_x.groupby(level=0)  \n",
    "        df_one_inj_x = grouped.last() \n",
    "\n",
    "        #date×検査項目にする\n",
    "        df_one_inj_x=df_one_inj_x.T\n",
    "        df_one_inj_x = df_one_inj_x.rename(index = {'有＝１': str(x)})\n",
    "        list_one_inj.append(df_one_inj_x)\n",
    "    merged_df_one_inj=pd.concat(list_one_inj)\n",
    "    all_data_inj.append(merged_df_one_inj)\n",
    "\n",
    "all_data_inj[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_inj_codes = clean_df_inj_opz['薬価基準収載医薬品コード最初７桁'].value_counts()\n",
    "c_inj_codes = pd.DataFrame(c_inj_codes)\n",
    "#####injコード100回以上登場する項目のみ採用する\n",
    "#inj_codesは使うinjの項目たち\n",
    "inj_codes = c_inj_codes.index[c_inj_codes['薬価基準収載医薬品コード最初７桁'] > 100]\n",
    "inj_codes = list(inj_codes)\n",
    "inj_codes\n",
    "\n",
    "df_inj_codes = pd.DataFrame(columns = inj_codes)\n",
    "df_inj_codes\n",
    "for x in range(447):\n",
    "    all_data_inj[x] = pd.concat([df_inj_codes, all_data_inj[x]], join = 'inner')\n",
    "all_data_inj[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labきれいにしたい\n",
    "\n",
    "#aaa = df_lab_opz[df_lab_opz['検査項目'] == '測定機器']\n",
    "#aaa['検査項目コード'].value_counts()\n",
    "##検査項目が測定機器のうち検査項目コード内訳は\n",
    "###0270100:\n",
    "###4315000\n",
    "###0259000\n",
    "###4325000\n",
    "###9329090\n",
    "#検査項目が測定機器なのを全部消す\n",
    "sokuteikiki_index = df_lab_opz.index[df_lab_opz['検査項目'] == '測定機器']\n",
    "sokuteikiki = df_lab_opz.drop(sokuteikiki_index)\n",
    "df_lab_opz = sokuteikiki\n",
    "\n",
    "def preprocess_df_lab_opz(df_lab_opz):\n",
    "    #labきれいにしたい\n",
    "    clean_df_lab_opz =  df_lab_opz.drop(['Time', '検査材料', '検査項目', 'フリーコメント','Value'], axis = 1)\n",
    "    clean_df_lab_opz = clean_df_lab_opz.sort_values(['PT_ID', 'Date', '検査項目コード'])\n",
    "\n",
    "    return df_lab_opz\n",
    "\n",
    "\n",
    "clean_df_lab_opz=preprocess_df_lab_opz(df_lab_opz)\n",
    "\n",
    "### 並列化高速版\n",
    "###\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def process_one_lab(x):\n",
    "    print(\"Processing: \",x)\n",
    "    #患者一人ぶん\n",
    "    df_one_lab = clean_df_lab_opz[clean_df_lab_opz[\"PT_ID\"]==str(x)]\n",
    "\n",
    "    #dateをリストに変換\n",
    "    #そして重複を削除\n",
    "    #並び替え\n",
    "    list_one_lab_date = list(set(df_one_lab['Date']))\n",
    "    list_one_lab_date.sort()\n",
    "\n",
    "    #list_dateは仮　本当はlist_one_lab_date\n",
    "    list_date = list_one_lab_date #['2009-08-20', '2009-10-07']\n",
    "    list_one_lab = []\n",
    "    for x in list_date:\n",
    "        #Date=xの時のデータ\n",
    "        df_one_x = df_one_lab[df_one_lab['Date']==x]\n",
    "        df_one_x = df_one_x.drop(['PT_ID', 'Date','Time'], axis = 1)\n",
    "        df_one_x = df_one_x.set_index(\"検査項目コード\")\n",
    "        ## 二回検査している？場合の処理\n",
    "        grouped = df_one_x.groupby(level=0)  \n",
    "        df_one_x = grouped.last()\n",
    "        #df_one_x[\"異常値\"]\n",
    "        df_one_x = df_one_x[[\"異常値\"]].T\n",
    "\n",
    "        #date×検査項目にする\n",
    "        df_one_x = df_one_x.rename(index = {'異常値': ''+str(x)+''})\n",
    "        list_one_lab.append(df_one_x)\n",
    "    merged_df_one_lab=pd.concat(list_one_lab)\n",
    "    return merged_df_one_lab\n",
    "\n",
    "clean_df_lab_opz=preprocess_df_lab_opz(df_lab_opz)\n",
    "p = Pool(32) # 並列プロセス数を設定\n",
    "all_data_lab = p.map(process_one_lab, list_PT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##lab頻出１００で相関\n",
    "#相関係数を算出してabs0.7より大きい項目は相関が高いということで除く\n",
    "lab_head = df_lab_opz['検査項目コード'].value_counts()\n",
    "df_lab_head = pd.DataFrame(lab_head)\n",
    "df_lab_head = df_lab_head.head(200)\n",
    "list_lab_head = df_lab_head.index\n",
    "list_lab_head\n",
    "\n",
    "lab100 = []\n",
    "for x in range(447):\n",
    "    #labdataからよくある検査項目コードhead100を抽出したい\n",
    "    lablab_data = all_data_lab[x]\n",
    "    lablab_data = lablab_data.filter(items = list_lab_head)\n",
    "    #＋L:-1、＋H:1、-:0にする\n",
    "    lablab_data = lablab_data.replace({'＋Ｈ':1, '−':0, '＋Ｌ':-1})\n",
    "    \n",
    "    lab100.append(lablab_data)\n",
    "\n",
    "all_data_lab_merge = pd.concat(lab100)\n",
    "\n",
    "#lab相関見たい\n",
    "#自信なし\n",
    "df_lab_corr = all_data_lab_merge.corr()\n",
    "print(df_lab_corr)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 9)) \n",
    "sns.heatmap(df_lab_corr, square=True, vmax=1, vmin=-1, center=0)\n",
    "plt.savefig('pt_id/lab100sokan.png')\n",
    "##相関高い特徴量を除外したい\n",
    "##相関係数0.7≦|r|≦1.0は強い相関があるとして片割れは除外\n",
    "import math\n",
    "threshold = 0.7 #しきい値\n",
    "# 相関行列を求める\n",
    "lab_corr = all_data_lab_merge.corr()\n",
    "print(lab_corr)\n",
    "\n",
    "while True:\n",
    "    columns = lab_corr.columns \n",
    "    data_max = 0.0\n",
    "    index_max  = None\n",
    "    column_max = None\n",
    "    #相関行列の中で最も相関の高い組を1つ取り出す。\n",
    "    for index in columns:\n",
    "        for column in columns:\n",
    "            data = abs(lab_corr.loc[index, column])\n",
    "            # 自身同士でなく、しきい値を超えているか。\n",
    "            if index != column and not np.isnan(data) and data > threshold :\n",
    "                #相関の最も高いものを記憶\n",
    "                if data_max < data:\n",
    "                    data_max = data\n",
    "                    index_max = index \n",
    "                    column_max = column \n",
    "\n",
    "    if data_max == 0.0:\n",
    "        # しきい値を超えるものがなければ終了\n",
    "        break\n",
    "    else:\n",
    "        # しきい値を超えるものがある場合、column側の特徴を相関行列から除去(行、列共に)\n",
    "        lab_corr.drop([column_max], axis=0, inplace=True)\n",
    "        lab_corr.drop([column_max], axis=1, inplace=True)\n",
    "\n",
    "lab_corr  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in list_lab_head_corr:\n",
    "    #toptop = all_data_lab_merge[str(i)].value_counts()\n",
    "    #print(toptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_lab_head_corr"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_data_lab_merge\n",
    "pd.set_option('display.max_rows', 307)\n",
    "pd.set_option('display.max_columns', 108)\n",
    "#MinMax法\n",
    "mm = preprocessing.MinMaxScaler()\n",
    "ttt = all_data_lab_merge\n",
    "ttt_mm = mm.fit_transform(ttt)\n",
    "\n",
    "ttt_mm = pd.DataFrame(ttt_mm, columns = list_lab_head)\n",
    "ttt_mm\n",
    "ppp = all_data_lab_merge.reset_index(drop=True)\n",
    "ppp = ppp.drop(list_lab_head, axis = 1)\n",
    "pt_mmm = pd.concat([ttt_mm, ppp], axis = 1)\n",
    "all_data_lab_merge = pt_mmm\n",
    "all_data_lab_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#頻出１００labの相関0.7以上を除いた94のtop50出す\n",
    "#異常多いtop50と普通top50全く同じだったから、普通のtop50でやってる\n",
    "##94項目のリスト\n",
    "list_lab_corr = lab_corr.columns.tolist()\n",
    "#DKFに入れたいlabのデータ\n",
    "#検査項目コード多い上位100個のリストを作りたい\n",
    "#lab_head = df_lab_opz['検査項目コード'].value_counts()\n",
    "#df_lab_head = pd.DataFrame(lab_head)\n",
    "#df_lab_head_corr = pd.DataFrame(index = list_lab_corr)\n",
    "#df_lab_head_corr = pd.concat([df_lab_head, df_lab_head_corr], axis = 0, join = 'inner')\n",
    "\n",
    "#df_lab_head_corr = df_lab_head_corr.head(50)\n",
    "#list_lab_head_corr = df_lab_head_corr.index\n",
    "#list_lab_head_corr\n",
    "#labデータ異常値が＋のもののみいったん出して見る\n",
    "#異常値が多いものです\n",
    "df_lab_opz_H = df_lab_opz[df_lab_opz['異常値'] == '＋Ｈ']\n",
    "df_lab_opz_L = df_lab_opz[df_lab_opz['異常値'] == '＋Ｌ']\n",
    "df_lab_opz_HL = pd.concat([df_lab_opz_H, df_lab_opz_L])\n",
    "vc_df_lab_opz_HL = df_lab_opz_HL['検査項目コード'].value_counts()\n",
    "vc_df_lab_opz_HL = pd.DataFrame(vc_df_lab_opz_HL)\n",
    "vc_df_lab_opz_HL\n",
    "df_lab_head_corr = pd.DataFrame(index = list_lab_corr)\n",
    "df_lab_head_corr = pd.concat([vc_df_lab_opz_HL, df_lab_head_corr], axis = 0, join = 'inner')\n",
    "\n",
    "df_lab_head_corr = df_lab_head_corr.head(50)\n",
    "list_lab_head_corr = df_lab_head_corr.index\n",
    "list_lab_head_corr\n",
    "df_lab_head_corr\n",
    "\n",
    "\n",
    "\n",
    "lab50_corr = []\n",
    "for x in range(447):\n",
    "    #labdataからよくある検査項目コードhead100を抽出したい\n",
    "    lablab_data = all_data_lab[x]\n",
    "    lablab_data = lablab_data.filter(items = list_lab_head_corr)\n",
    "    #＋L:-1、＋H:1、-:0にする##MMscaler考慮\n",
    "    lablab_data = lablab_data.replace({'＋Ｈ':1, '−':0.5, '＋Ｌ':0})\n",
    "    list_columns = lablab_data.columns.tolist()\n",
    "    for i in list_columns:\n",
    "        df_bool = (lablab_data[str(i)] == 0)\n",
    "        counts_df_bool = df_bool.sum()\n",
    "        if counts_df_bool < 0:\n",
    "            lablab_data[str(i)] = lablab_data[str(i)].replace(0.5, 0)\n",
    "        else:\n",
    "            lablab_data[str(i)] = lablab_data[str(i)]\n",
    "    lab50_corr.append(lablab_data)\n",
    "    \n",
    "lab50_corr[1]\n",
    "#list_lab_head_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_lab_head_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#検査項目コードと検査項目の対応の表作ります\n",
    "##検査項目コードと検査項目dfから抽出\n",
    "k_df_lab_opz = df_lab_opz.loc[:,['検査項目コード','検査項目']]\n",
    "k_df_lab_opz.drop_duplicates(subset = ['検査項目コード', '検査項目'], inplace = True)\n",
    "list_k_df_lab_opz_one = []\n",
    "for i in list_lab_head_corr:\n",
    "    k_df_lab_opz_one = k_df_lab_opz[k_df_lab_opz[\"検査項目コード\"] == str(i)]\n",
    "    list_k_df_lab_opz_one.append(k_df_lab_opz_one)\n",
    "k_merge_df_lab_opz = pd.concat(list_k_df_lab_opz_one)\n",
    "############################\n",
    "#用いている検査項目コードとの対応\n",
    "#k_merge_df_lab_opz.to_csv(\"labcode_50.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_merge_df_lab_opz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nameきれいにしたい\n",
    "def preprocess_df_name_opz(df_name_opz):\n",
    "    clean_df_name_opz =  df_name_opz.drop(['診療科コード', '診療科', '接頭語１', '接頭語２','接尾語１','接尾語２','ICD10コード(基礎疾患)', '病名', '疑い対象', '病名(＋修飾語)','転帰区分','転帰日'], axis = 1)\n",
    "    #kラム名を検査コードと区別できるように変更\n",
    "    list_namecode = [clean_df_name_opz['病名コード']]\n",
    "    for x in list_namecode:\n",
    "        list_namecode_ = '病名'+ x\n",
    " \n",
    "    clean_df_name_opz = clean_df_name_opz.drop(['病名コード'], axis = 1)\n",
    "    clean_df_name_opz['病名コード'] = list_namecode_\n",
    "    \n",
    "    clean_df_name_opz['有＝１'] = 1\n",
    "    \n",
    "    return clean_df_name_opz\n",
    "\n",
    "clean_df_name_opz = preprocess_df_name_opz(df_name_opz)\n",
    "#PT_IDぶんのforループ\n",
    "all_data_name = []\n",
    "for x in list_PT_ID:\n",
    "    df_one_name = clean_df_name_opz[clean_df_name_opz[\"PT_ID\"] == str(x)]\n",
    "    \n",
    "    #dateをリストに変換、重複削除、並び替え\n",
    "    list_one_name_date = list(set(df_one_name['Date']))\n",
    "    list_one_name_date.sort()\n",
    "    \n",
    "    list_date_name = list_one_name_date\n",
    "    list_one_name = []\n",
    "    \n",
    "    for x in list_date_name:\n",
    "        #Date = xの時のデータ\n",
    "        df_one_name_x = df_one_name[df_one_name['Date'] == str(x)]\n",
    "        df_one_name_x = df_one_name_x.drop(['PT_ID', 'Date'], axis = 1)\n",
    "        df_one_name_x = df_one_name_x.set_index(\"病名コード\")\n",
    "        \n",
    "        ##2回診断されている場合？の処理\n",
    "        grouped = df_one_name_x.groupby(level = 0)\n",
    "        df_one_name_x = grouped.last()\n",
    "\n",
    "        \n",
    "        #date×病名コードにする\n",
    "        df_one_name_x = df_one_name_x.T\n",
    "        df_one_name_x = df_one_name_x.rename(index = {'有＝１': str(x)})\n",
    "        list_one_name.append(df_one_name_x)\n",
    "    \n",
    "    merged_df_one_name = pd.concat(list_one_name)\n",
    "    all_data_name.append(merged_df_one_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#病名の中から癌のみをピックアップ\n",
    "df_name_opz['病名'].value_counts()\n",
    "#icd=cが悪性新生物\n",
    "\n",
    "df_name_cancer = df_name_opz[df_name_opz['ICD10コード(基礎疾患)'].str.startswith('C', na = False)]\n",
    "print(df_name_cancer['ICD10コード(基礎疾患)'].value_counts())\n",
    "\n",
    "#病名はICDの頭文字で分類する\n",
    "df_name_opz\n",
    "##ICDコード不明（302個）はいったん無視する～～～\n",
    "#print(df_name_opz.isnull().sum())\n",
    "\n",
    "#list_icd_x = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'N', 'M', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "#icd_name = []\n",
    "#for x in list_icd_x:\n",
    "    #icd_x = df_name_opz[df_name_opz['ICD10コード(基礎疾患)'].str.startswith(str(x), na = False)]\n",
    "    #icd_x['ICD-X'] = str(x)\n",
    "    #icd_name.append(icd_x)    \n",
    "    \n",
    "#icd_name = pd.concat([icd_name[0],icd_name[1],icd_name[2],icd_name[3],icd_name[4],icd_name[5],icd_name[6],icd_name[7],icd_name[8],icd_name[9],icd_name[10],icd_name[11],icd_name[12],icd_name[13],icd_name[14],icd_name[15],icd_name[16],icd_name[17],icd_name[18],icd_name[19],icd_name[20],icd_name[21],icd_name[22],icd_name[23],icd_name[24],icd_name[25]], axis = 0, join = 'outer')\n",
    "#icd_name\n",
    "\n",
    "icd_x = df_name_opz[df_name_opz['ICD10コード(基礎疾患)'].str.startswith('C', na = False)]\n",
    "icd_x \n",
    "\n",
    "icd_x['ICD10コード(基礎疾患)最初3桁'] = icd_x['ICD10コード(基礎疾患)'].str[:3]\n",
    "\n",
    "##C頭文字２個だと６３種と多いので（腫瘍リスト）作成\n",
    "#あほみたいなことしているのは後で項目変えたいってなったと時用です\n",
    "list_icd_x = pd.DataFrame(icd_x['ICD10コード(基礎疾患)最初3桁'].value_counts())\n",
    "list_icd_x = list(list_icd_x.index)\n",
    "list_icd_x = sorted(list_icd_x)\n",
    "list_icd_x\n",
    "list_c00_14 = ['C02','C03','C04','C05','C06','C07','C08','C10','C11','C13','C14']\n",
    "list_c15_26 = ['C15','C16','C17','C18','C20','C22','C23','C24','C25','C26']\n",
    "list_c30_39 = ['C30','C31','C32','C34','C38']\n",
    "list_c40_41 = ['C41']\n",
    "list_c43_44 = ['C43','C44']\n",
    "list_c45_49 = ['C45','C48','C49']\n",
    "list_c50 = ['C50']\n",
    "list_c51_58 = ['C51','C52','C53','C54','C55','C56','C58']\n",
    "list_c60_63 = ['C61']\n",
    "list_c64_68 = ['C64','C65','C66','C67','C68']\n",
    "list_c69_72 = ['C69','C71']\n",
    "list_c73_75 = ['C73','C74']\n",
    "list_c76_80 = ['C76','C77','C78','C79','C80']\n",
    "list_c81_96 = ['C81','C83','C84','C85','C90','C91','C92','C95']\n",
    "\n",
    "list_icd_c00_14 = []\n",
    "for x in list_c00_14:\n",
    "    list_icd_c00_14.append(icd_x[icd_x['ICD10コード(基礎疾患)最初3桁'] == str(x)])\n",
    "df_icd_c00_14 = pd.concat(list_icd_c00_14)\n",
    "df_icd_c00_14['がんグループ'] = 'c00_14'\n",
    "\n",
    "list_icd_c15_26 = []\n",
    "for x in list_c15_26:\n",
    "    list_icd_c15_26.append(icd_x[icd_x['ICD10コード(基礎疾患)最初3桁'] == str(x)])\n",
    "df_icd_c15_26 = pd.concat(list_icd_c15_26)\n",
    "df_icd_c15_26['がんグループ'] = 'c15_26'\n",
    "\n",
    "list_icd_c30_39 = []\n",
    "for x in list_c30_39:\n",
    "    list_icd_c30_39.append(icd_x[icd_x['ICD10コード(基礎疾患)最初3桁'] == str(x)])\n",
    "df_icd_c30_39 = pd.concat(list_icd_c30_39)\n",
    "df_icd_c30_39['がんグループ'] = 'c30_39'\n",
    " \n",
    "list_icd_c40_41 = []\n",
    "for x in list_c40_41:\n",
    "    list_icd_c40_41.append(icd_x[icd_x['ICD10コード(基礎疾患)最初3桁'] == str(x)])\n",
    "df_icd_c40_41 = pd.concat(list_icd_c40_41)\n",
    "df_icd_c40_41['がんグループ'] = 'c40_41'\n",
    "\n",
    "list_icd_c43_44 = []\n",
    "for x in list_c43_44:\n",
    "    list_icd_c43_44.append(icd_x[icd_x['ICD10コード(基礎疾患)最初3桁'] == str(x)])\n",
    "df_icd_c43_44 = pd.concat(list_icd_c43_44)\n",
    "df_icd_c43_44['がんグループ'] = 'c43_44'\n",
    "\n",
    "list_icd_c45_49 = []\n",
    "for x in list_c45_49:\n",
    "    list_icd_c45_49.append(icd_x[icd_x['ICD10コード(基礎疾患)最初3桁'] == str(x)])\n",
    "df_icd_c45_49 = pd.concat(list_icd_c45_49)\n",
    "df_icd_c45_49['がんグループ'] = 'c45_49'\n",
    "\n",
    "list_icd_c50 = []\n",
    "for x in list_c50:\n",
    "    list_icd_c50.append(icd_x[icd_x['ICD10コード(基礎疾患)最初3桁'] == str(x)])\n",
    "df_icd_c50 = pd.concat(list_icd_c50)\n",
    "df_icd_c50['がんグループ'] = 'c50'\n",
    "\n",
    "list_icd_c51_58 = []\n",
    "for x in list_c51_58:\n",
    "    list_icd_c51_58.append(icd_x[icd_x['ICD10コード(基礎疾患)最初3桁'] == str(x)])\n",
    "df_icd_c51_58 = pd.concat(list_icd_c51_58)\n",
    "df_icd_c51_58['がんグループ'] = 'c51_58'\n",
    "\n",
    "list_icd_c60_63 = []\n",
    "for x in list_c60_63:\n",
    "    list_icd_c60_63.append(icd_x[icd_x['ICD10コード(基礎疾患)最初3桁'] == str(x)])\n",
    "df_icd_c60_63 = pd.concat(list_icd_c60_63)\n",
    "df_icd_c60_63['がんグループ'] = 'c60_63'\n",
    "\n",
    "list_icd_c64_68 = []\n",
    "for x in list_c64_68:\n",
    "    list_icd_c64_68.append(icd_x[icd_x['ICD10コード(基礎疾患)最初3桁'] == str(x)])\n",
    "df_icd_c64_68 = pd.concat(list_icd_c64_68)\n",
    "df_icd_c64_68['がんグループ'] = 'c64_68'\n",
    "\n",
    "list_icd_c69_72 = []\n",
    "for x in list_c69_72:\n",
    "    list_icd_c69_72.append(icd_x[icd_x['ICD10コード(基礎疾患)最初3桁'] == str(x)])\n",
    "df_icd_c69_72 = pd.concat(list_icd_c69_72)\n",
    "df_icd_c69_72['がんグループ'] = 'c69_72'\n",
    "\n",
    "list_icd_c73_75 = []\n",
    "for x in list_c73_75:\n",
    "    list_icd_c73_75.append(icd_x[icd_x['ICD10コード(基礎疾患)最初3桁'] == str(x)])\n",
    "df_icd_c73_75 = pd.concat(list_icd_c73_75)\n",
    "df_icd_c73_75['がんグループ'] = 'c73_75'\n",
    "\n",
    "list_icd_c76_80 = []\n",
    "for x in list_c76_80:\n",
    "    list_icd_c76_80.append(icd_x[icd_x['ICD10コード(基礎疾患)最初3桁'] == str(x)])\n",
    "df_icd_c76_80 = pd.concat(list_icd_c76_80)\n",
    "df_icd_c76_80['がんグループ'] = 'c76_80'\n",
    "\n",
    "list_icd_c81_96 = []\n",
    "for x in list_c81_96:\n",
    "    list_icd_c81_96.append(icd_x[icd_x['ICD10コード(基礎疾患)最初3桁'] == str(x)])\n",
    "df_icd_c81_96 = pd.concat(list_icd_c81_96)\n",
    "df_icd_c81_96['がんグループ'] = 'c81_96'\n",
    "\n",
    "\n",
    "list_icd_xx = [df_icd_c00_14, df_icd_c15_26, df_icd_c30_39, df_icd_c40_41, df_icd_c43_44, df_icd_c45_49, df_icd_c50, df_icd_c51_58, df_icd_c60_63, df_icd_c64_68, df_icd_c69_72, df_icd_c73_75, df_icd_c76_80, df_icd_c81_96]\n",
    "icd_x = pd.concat(list_icd_xx)\n",
    "\n",
    "icd_name = icd_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#icd_nameきれいにしたい\n",
    "def preprocess_icd_name_opz(icd_name):\n",
    "    clean_icd_name_opz =  icd_name.drop(['診療科コード', 'ICD10コード(基礎疾患)最初3桁', '診療科', '病名コード', '接頭語１', '接頭語２','接尾語１','接尾語２','ICD10コード(基礎疾患)', '病名', '疑い対象', '病名(＋修飾語)','転帰区分','転帰日'], axis = 1)\n",
    "    \n",
    "    clean_icd_name_opz['有＝１'] = 1\n",
    "    \n",
    "    return clean_icd_name_opz\n",
    "\n",
    "clean_icd_name_opz = preprocess_icd_name_opz(icd_name)\n",
    "\n",
    "#都合上、PT_ID、Date、ICD-X　同じ行削除\n",
    "clean_icd_name_opz.drop_duplicates(subset = ['PT_ID', 'Date', 'がんグループ'], inplace = True)\n",
    "\n",
    "#PT_IDぶんのforループ\n",
    "all_data_icd = []\n",
    "for x in list_PT_ID:\n",
    "    df_one_icd = clean_icd_name_opz[clean_icd_name_opz[\"PT_ID\"] == str(x)]\n",
    "    \n",
    "    #dateをリストに変換、重複削除、並び替え\n",
    "    list_one_icd_date = list(set(df_one_icd['Date']))\n",
    "    list_one_icd_date.sort()\n",
    "    list_date_icd = list_one_icd_date\n",
    "    \n",
    "    list_one_icd = []\n",
    "    for x in list_date_icd:\n",
    "        #Date = xの時のデータ\n",
    "        df_one_icd_x = df_one_icd[df_one_icd['Date'] == str(x)]\n",
    "        df_one_icd_x = df_one_icd_x.drop(['PT_ID', 'Date'], axis = 1)\n",
    "        df_one_icd_x = df_one_icd_x.set_index(\"がんグループ\")\n",
    "    \n",
    "        #date×ICD-Xにする\n",
    "        df_one_icd_x = df_one_icd_x.T\n",
    "        df_one_icd_x = df_one_icd_x.rename(index = {'有＝１': str(x)})\n",
    "        list_one_icd.append(df_one_icd_x)\n",
    "        #2回診断されている場合？の処理\n",
    "        icd_grouped = df_one_icd_x.groupby(level = 0)\n",
    "        df_one_icd_x = icd_grouped.last()\n",
    "\n",
    "    merged_df_one_icd = pd.concat(list_one_icd)\n",
    "    all_data_icd.append(merged_df_one_icd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_icd_merge = pd.concat(all_data_icd)\n",
    "all_data_icd_notnull = all_data_icd_merge.notnull().sum()\n",
    "c_icd_codes = all_data_icd_notnull.sort_values()\n",
    "\n",
    "c_icd_codes = pd.DataFrame(c_icd_codes)\n",
    "c_icd_codes\n",
    "#####injコード100回以上登場する項目のみ採用する\n",
    "#inj_codesは使うinjの項目たち\n",
    "icd_codes = c_icd_codes.index[c_icd_codes[0] > 100]\n",
    "icd_codes = list(icd_codes)\n",
    "icd_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_icd_codes = pd.DataFrame(columns = icd_codes)\n",
    "df_icd_codes\n",
    "for x in range(447):\n",
    "    all_data_icd[x] = pd.concat([df_icd_codes, all_data_icd[x]], join = 'inner')\n",
    "all_data_icd[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_c60_63_name = []\n",
    "list_c64_68_name = []\n",
    "list_c43_44_name = []\n",
    "list_c00_14_name = []\n",
    "\n",
    "list_c81_96_name = []\n",
    "list_c15_26_name = []\n",
    "list_c30_39_name = []\n",
    "list_c76_80_name = []\n",
    "\n",
    "for i in range(447):\n",
    "    all_data_icd_one = all_data_icd[i]\n",
    "    if 'c60_63' in all_data_icd_one.columns:\n",
    "        list_c60_63_name.append(i)\n",
    "    if 'c64_68' in all_data_icd_one.columns:\n",
    "        list_c64_68_name.append(i)\n",
    "    if 'c43_44' in all_data_icd_one.columns:\n",
    "        list_c43_44_name.append(i)\n",
    "    if 'c00_14' in all_data_icd_one.columns:\n",
    "        list_c00_14_name.append(i)\n",
    "        \n",
    "    if 'c81_96' in all_data_icd_one.columns:\n",
    "        list_c81_96_name.append(i)\n",
    "    if 'c15_26' in all_data_icd_one.columns:\n",
    "        list_c15_26_name.append(i)\n",
    "    if 'c30_39' in all_data_icd_one.columns:\n",
    "        list_c30_39_name.append(i)\n",
    "    if 'c76_80' in all_data_icd_one.columns:\n",
    "        list_c76_80_name.append(i)\n",
    "    \n",
    "len(list_c76_80_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age_opz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age_droupby = df_age_opz.groupby('PT_ID').agg({'estim_Age_groupnum': 'median'})\n",
    "df_age_droupby = df_age_droupby.reset_index()\n",
    "df_age_droupby.sort_values('PT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_age_droupby_1 = df_age_droupby[df_age_droupby['estim_Age_groupnum'] <2]\n",
    "list_1 = df_age_droupby_1.index\n",
    "df_age_droupby_2 = df_age_droupby.query('2<= estim_Age_groupnum <3')\n",
    "list_2 = df_age_droupby_2.index\n",
    "df_age_droupby_3 = df_age_droupby.query('3<= estim_Age_groupnum <4')\n",
    "list_3 = df_age_droupby_3.index\n",
    "df_age_droupby_4 = df_age_droupby.query('4<= estim_Age_groupnum <5')\n",
    "list_4 = df_age_droupby_4.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#age正規化\n",
    "df_age_opz\n",
    "#MinMax法\n",
    "mm = preprocessing.MinMaxScaler()\n",
    "\n",
    "ttt = df_age_opz\n",
    "ttt = ttt.values[:, 3:4]\n",
    "ttt_mm = mm.fit_transform(ttt)\n",
    "list_columns_age = ['estim_Age_groupnum']\n",
    "ttt_mm = pd.DataFrame(ttt_mm, columns = list_columns_age)\n",
    "ttt_mm\n",
    "ppp = df_age_opz.reset_index(drop=True)\n",
    "ppp = ppp.drop(list_columns_age, axis = 1)\n",
    "pt_mmm = pd.concat([ttt_mm, ppp], axis = 1)\n",
    "df_age_opz = pt_mmm\n",
    "df_age_opz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hw_opz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hw正規化\n",
    "df_hw_opz\n",
    "#MinMax法\n",
    "mm = preprocessing.MinMaxScaler()\n",
    "\n",
    "ttt = df_hw_opz\n",
    "ttt = ttt.values[:, 2:4]\n",
    "ttt_mm = mm.fit_transform(ttt)\n",
    "list_columns_hw = ['身長', '体重']\n",
    "ttt_mm = pd.DataFrame(ttt_mm, columns = list_columns_hw)\n",
    "ttt_mm\n",
    "ppp = df_hw_opz.reset_index(drop=True)\n",
    "ppp = ppp.drop(list_columns_hw, axis = 1)\n",
    "pt_mmm = pd.concat([ttt_mm, ppp], axis = 1)\n",
    "df_hw_opz = pt_mmm\n",
    "df_hw_opz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ageきれいにしたい\n",
    "all_data_age = []\n",
    "for x in list_PT_ID:\n",
    "    df_one_age = df_age_opz[df_age_opz[\"PT_ID\"] == str(x)]\n",
    "    df_one_age_x = df_one_age.drop(['PT_ID', 'index'], axis = 1)\n",
    "    \n",
    "    all_data_age.append(df_one_age_x)\n",
    "#hwきれいにしたい\n",
    "all_data_hw = []\n",
    "for x in list_PT_ID:\n",
    "    df_one_hw = df_hw_opz[df_hw_opz[\"PT_ID\"] == str(x)]\n",
    "    df_one_hw_x = df_one_hw.drop(['PT_ID'], axis = 1)\n",
    "    \n",
    "    all_data_hw.append(df_one_hw_x)\n",
    "#性別\n",
    "df_sex_opz  = df_sex_opz.sort_values('PT_ID')\n",
    "list_sex_opz = df_sex_opz['estim_bln_Male'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "columns作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Dateを1回目二回目的な概念でとらえる？ためにタイムポイント（仮）を作る\n",
    "#今回最もDate多いので752日だったので１～７５２のdf作成\n",
    "##下位90パーで日数をカットすることにしました307日\n",
    "lis_empty_timepoint = list(range(1, 307+1))\n",
    "df_empty_timepoint = pd.DataFrame(columns = ['time_point_number'])\n",
    "df_empty_timepoint['time_point_number'] = lis_empty_timepoint\n",
    "#df_empty_timepoint\n",
    "\n",
    "list_else = ['身長', '体重', 'estim_Age_groupnum', '体温', '脈拍', '血圧(最高)', '血圧(最低)', 'estim_bln_Male']\n",
    "#カラムsから空のデータフレームを作成する\n",
    "#hw,age.vital.sex.lab.inj,nameよりcolumns作成\n",
    "#list_lab_head = list(list_lab_head)\n",
    "#list_c_name_head = list(list_c_name_head)\n",
    "list_lab_head_corr = list(list_lab_head_corr)\n",
    "#薬価基準収載7桁のリスト作成\n",
    "yyy = clean_df_inj_opz['薬価基準収載医薬品コード最初７桁'].value_counts()\n",
    "yyy = pd.DataFrame(yyy)\n",
    "list_inj7 = list(yyy.index)\n",
    "#がん病名 ICD10コード(基礎疾患)最初3桁\n",
    "zzz = clean_icd_name_opz['がんグループ'].value_counts()\n",
    "zzz = pd.DataFrame(zzz)\n",
    "list_icd = list(zzz.index)\n",
    "\n",
    "\n",
    "list_inj7 = inj_codes\n",
    "list_inj_7= inj_codes\n",
    "list_icd = icd_codes\n",
    "\n",
    "#list_empty_columns = list_else + list_lab_head + list_inj7 + list_icd\n",
    "list_empty_columns = list_else + list_lab_head_corr + list_inj7 + list_icd\n",
    "list_empty_columns\n",
    "\n",
    "df_list_empty_columns = pd.DataFrame(columns = list_empty_columns)\n",
    "df_list_empty_columns\n",
    "##hw2,age1,vital4,sex1,lab100,inj34,name26の168カラムになりました"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mergeする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "##カラム減らした完全バージョン\n",
    "##all_data_と欠損値補完方法が異なる\n",
    "#欠損は前の値で埋める\n",
    "#ごみすぎる。。つら。\n",
    "all_data = []\n",
    "all_data_df = []\n",
    "for x in range(447):\n",
    "    pt_3 = pd.concat([lab50_corr[x], all_data_inj[x], all_data_icd[x]], axis = 1, join = 'outer')\n",
    "    pt_1  = pd.merge(all_data_hw[x],all_data_age[x], on = 'Date', how = 'outer')\n",
    "    pt_2 = pd.merge(pt_1,all_data_vital[x], on = 'Date', how = 'outer')\n",
    "    pt_2 = pt_2.sort_values(['Date'])\n",
    "    pt_3['Date'] = pt_3.index\n",
    "    pt_3['Date'] = pd.to_datetime(pt_3['Date'])\n",
    "    pt_3 = pt_3.sort_values(['Date'])\n",
    "\n",
    "    pt_  = pd.merge(pt_2, pt_3, on = 'Date', how = 'outer')\n",
    "\n",
    "    ##性別追加\n",
    "    pt_[\"estim_bln_Male\"] = list_sex_opz[x]\n",
    "    pt_ = pt_.sort_values(['Date'])\n",
    "\n",
    "    ##すべての項目追加\n",
    "    pt_ = pd.concat([pt_, df_list_empty_columns], axis = 0, join = 'outer')\n",
    "\n",
    "    ##タイムポイントを300日に設定しているので300より多い場合は早い日付けをカットする\n",
    "    if len(pt_) > 307:\n",
    "        pt_ = pt_.tail(307)\n",
    "    else:\n",
    "        pt_ = pt_\n",
    "        \n",
    "    #タイムポイント（仮）を振る\n",
    "    xxx = len(pt_)\n",
    "    lis_xxx = list(range(1, xxx+1))\n",
    "    pt_[\"time_point_number\"] = lis_xxx\n",
    "\n",
    "    #タイムポイントを振った患者dfとタイムポイントのdfとタイムポイントをkeyに結合\n",
    "    pt_ = pd.merge(pt_, df_empty_timepoint, on = 'time_point_number', how = 'outer')\n",
    "\n",
    "    #timepointを消す\n",
    "    pt_ = pt_.drop(\"time_point_number\", axis = 1)\n",
    "    \n",
    "\n",
    "    #～～～処理～～～\n",
    "    #age標準化\n",
    "    #欠損ありなしに分けて、なしで標準化し結合\n",
    "    #yyy = pt_['estim_Age_groupnum']\n",
    "    #yyy_notnan = yyy[yyy.notnull()]\n",
    "    #yyy_nan = yyy[yyy.isnull()]\n",
    "    #yyy_notnan = (yyy_notnan - yyy_notnan.mean()) / yyy_notnan.std()\n",
    "    #yyy_notnan = pd.DataFrame(yyy_notnan)\n",
    "    #yyy_nan = pd.DataFrame(yyy_nan)\n",
    "    #sc = StandardScaler()\n",
    "    #yyy_notnan.loc[:, :] = sc.fit_transform(yyy_notnan)\n",
    "    #yyy = pd.concat([yyy_notnan, yyy_nan])\n",
    "    #インデックスでソート\n",
    "    #yyy = yyy.sort_index()\n",
    "    #pt_ = pt_.drop(\"estim_Age_groupnum\", axis = 1)\n",
    "    #pt_ = pd.merge(pt_, yyy, left_index=True, right_index=True)\n",
    "    #age欠損は１つ前で埋める\n",
    "    pt_['estim_Age_groupnum'] = pt_['estim_Age_groupnum'].fillna(method = 'ffill')\n",
    "    #最初のデータより前はないから1番小さい値で埋めるで良きかな\n",
    "    pt_age_min = pt_['estim_Age_groupnum'].min()\n",
    "    pt_['estim_Age_groupnum'] = pt_['estim_Age_groupnum'].fillna(pt_age_min)\n",
    "\n",
    "    \n",
    "    #injの欠損値を０で埋める（欠損でない）\n",
    "    for i in list_inj7:\n",
    "        pt_[str(i)] = pt_[str(i)].fillna(0)\n",
    "        #injを一応floatへ\n",
    "        pt_[str(i)] = pt_[str(i)].astype('float')   \n",
    "        \n",
    "    #nameは1までを０、１からは全て１にする（欠損でない）\n",
    "    for n in list_icd:\n",
    "        pt_[str(n)] = pt_[str(n)].fillna(method = 'ffill')\n",
    "        pt_[str(n)] = pt_[str(n)].fillna(0)\n",
    "        #nameを一応floatへ\n",
    "        pt_[str(n)] = pt_[str(n)].astype('float')\n",
    "        #いったんタイムステップあるとこまででカットする\n",
    "        #len_steps = len(pt_['Date'])\n",
    "        #pt_steps = pt_.iloc[:len_steps+1, :]\n",
    "        #標準化\n",
    "        #yyy = pt_steps[str(n)]\n",
    "        #yyy_notnan = yyy[yyy.notnull()]\n",
    "        #yyy_nan = yyy[yyy.isnull()]\n",
    "        ##yyy_notnan = (yyy_notnan - yyy_notnan.mean()) / yyy_notnan.std()\n",
    "        #yyy_notnan = pd.DataFrame(yyy_notnan)\n",
    "        #yyy_nan = pd.DataFrame(yyy_nan)\n",
    "        #sc = StandardScaler()\n",
    "        #yyy_notnan.loc[:, :] = sc.fit_transform(yyy_notnan)\n",
    "        #yyy = pd.concat([yyy_notnan, yyy_nan])\n",
    "        #インデックスでソート\n",
    "        #yyy = yyy.sort_index()\n",
    "        #pt_ = pt_.drop(str(n), axis = 1)\n",
    "        #pt_ = pd.merge(pt_, yyy, left_index=True, right_index=True)\n",
    "        #一応step外も埋めておく\n",
    "        #pt_[str(n)] = pt_[str(n)].fillna(method = 'ffill')\n",
    "        \n",
    "        \n",
    "        \n",
    "    #身長は欠損でない\n",
    "    pt_['体重'] = pt_['体重'].astype(float)\n",
    "    pt_['身長'] = pt_['身長'].astype(float)\n",
    "    #身長、体重、vital標準化\n",
    "    #list_yyy = ['身長', '体重', '体温', '脈拍', '血圧(最高)', '血圧(最低)']\n",
    "    #for i in list_yyy:\n",
    "        #yyy = pt_[str(i)]\n",
    "        #yyy_notnan = yyy[yyy.notnull()]\n",
    "        #yyy_nan = yyy[yyy.isnull()]\n",
    "        ##yyy_notnan = (yyy_notnan - yyy_notnan.mean()) / yyy_notnan.std()\n",
    "        #yyy_notnan = pd.DataFrame(yyy_notnan)\n",
    "        #yyy_nan = pd.DataFrame(yyy_nan)\n",
    "        #sc = StandardScaler()\n",
    "        #yyy_notnan.loc[:, :] = sc.fit_transform(yyy_notnan)\n",
    "        #yyy = pd.concat([yyy_notnan, yyy_nan])\n",
    "        #インデックスでソート\n",
    "        #yyy = yyy.sort_index()\n",
    "        #pt_ = pt_.drop(str(i), axis = 1)\n",
    "        #pt_ = pd.merge(pt_, yyy, left_index=True, right_index=True)\n",
    "        \n",
    "    pt_['身長'] = pt_['身長'].fillna(method = 'ffill')    \n",
    "    pt_['身長'] = pt_['身長'].fillna(pt_['身長'].mean()) \n",
    "    \n",
    "    ##Dateの1個前のDateとの差Δtを求める\n",
    "    #Δt求める\n",
    "    pt_['Δt'] = (pt_['Date'].diff()).dt.days\n",
    "    pt_['Δt'] = pt_['Δt'].astype('float')\n",
    "    #Δt標準化する\n",
    "    #yyy = pt_['Δt']\n",
    "    #yyy_notnan = yyy[yyy.notnull()]\n",
    "    #yyy_nan = yyy[yyy.isnull()]\n",
    "    ##yyy_notnan = (yyy_notnan - yyy_notnan.mean()) / yyy_notnan.std()\n",
    "    #yyy_notnan = pd.DataFrame(yyy_notnan)\n",
    "    #yyy_nan = pd.DataFrame(yyy_nan)\n",
    "    #sc = StandardScaler()\n",
    "    #yyy_notnan.loc[:, :] = sc.fit_transform(yyy_notnan)\n",
    "    #yyy = pd.concat([yyy_notnan, yyy_nan])\n",
    "    #インデックスでソート\n",
    "    #yyy = yyy.sort_index()\n",
    "    #pt_ = pt_.drop(\"Δt\", axis = 1)\n",
    "    #pt_ = pd.merge(pt_, yyy, left_index=True, right_index=True)\n",
    "    \n",
    "    pt_ = pt_.drop(columns = 'Date')\n",
    "    \n",
    "    \n",
    "    #labをfloatへ\n",
    "    for j in list_lab_head_corr:\n",
    "        pt_[str(j)] = pt_[str(j)].astype('float')\n",
    "        #labを標準化する\n",
    "        #yyy = pt_[str(j)]\n",
    "        #yyy_notnan = yyy[yyy.notnull()]\n",
    "        #yyy_nan = yyy[yyy.isnull()]\n",
    "        ##yyy_notnan = (yyy_notnan - yyy_notnan.mean()) / yyy_notnan.std()\n",
    "        #yyy_notnan = pd.DataFrame(yyy_notnan)\n",
    "        #yyy_nan = pd.DataFrame(yyy_nan)\n",
    "        #if yyy.isnull().all():\n",
    "            #完全な欠損（1回もその検査を行っていない）は問題なくて検査しなかったのかなという判断で０埋めする。。。？\n",
    "            #pt_[str(j)] = 0\n",
    "        #else:\n",
    "            #sc = StandardScaler()\n",
    "            #yyy_notnan.loc[:, :] = sc.fit_transform(yyy_notnan)\n",
    "            #yyy = pd.concat([yyy_notnan, yyy_nan])\n",
    "            #インデックスでソート\n",
    "            #yyy = yyy.sort_index()\n",
    "            #pt_ = pt_.drop(str(j), axis = 1)\n",
    "            #pt_ = pd.merge(pt_, yyy, left_index=True, right_index=True)\n",
    " \n",
    " \n",
    "    \n",
    "    ##～～～～欠損処理～～～～\n",
    "    ##Date欠損どうしよう～～～とりあえず０で埋める\n",
    "    pt_['Δt'] = pt_['Δt'].fillna(0)\n",
    "    pt_ = pd.concat([df_list_empty_columns, pt_], axis = 0, join = 'outer')\n",
    "    #正規化\n",
    "    #MinMax法\n",
    "    mm = preprocessing.MinMaxScaler()\n",
    "\n",
    "    ttt = pt_\n",
    "    ttt = ttt.values[:, 80:81]\n",
    "    ttt_mm = mm.fit_transform(ttt)\n",
    "    list_columns_t = ['Δt']\n",
    "    ttt_mm = pd.DataFrame(ttt_mm, columns = list_columns_t)\n",
    "    ttt_mm\n",
    "    ppp = pt_.reset_index(drop=True)\n",
    "    ppp = ppp.drop(list_columns_t, axis = 1)\n",
    "    pt_mmm = pd.concat([ttt_mm, ppp], axis = 1)\n",
    "    pt_ = pt_mmm\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #lab\n",
    "    for j in list_lab_head_corr:\n",
    "        #欠損値を１つ前の値で埋める\n",
    "        #######↑NEW\n",
    "        pt_[str(j)] = pt_[str(j)].fillna(method = 'ffill')\n",
    "        #最初の値より前の欠損値は問題なしとして０で埋める\n",
    "        ######↑NEW\n",
    "        #完全な欠損（1回もその検査を行っていない）は問題なくて検査しなかったのかなという判断で０埋めする。。。？\n",
    "        \n",
    "        ###0.5埋め！！！\n",
    "        pt_[str(j)] = pt_[str(j)].fillna(0.5)\n",
    "    \n",
    "    ##欠損値を１つ前の値で埋める\n",
    "    pt_ = pt_.fillna(method = 'ffill') \n",
    "    #最初の欠損より前の値は平均で埋める\n",
    "    #(言っても体重とバイタルくらいかな)\n",
    "    pt_ = pt_.fillna(pt_.mean())\n",
    "    \n",
    "    ##標準化する（平均０標準偏差１になるようにする）\n",
    "    #sc = StandardScaler()\n",
    "    #pt_.loc[:, :] = sc.fit_transform(pt_) \n",
    "    pt_ = pd.concat([df_list_empty_columns, pt_], axis = 0, join = 'outer')\n",
    "    \n",
    "    pt_['Δt'] = pt_['Δt'].astype('float')\n",
    "    pt_['estim_bln_Male'] = pt_['estim_bln_Male'].astype('float')\n",
    "##numpy 3次元へ変換\n",
    "    all_data.append(pt_.values)\n",
    "    all_data_df.append(pt_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#alldataとall_data_verでmaskは同じだが一応！！！\n",
    "##カラム減らしたバージョン\n",
    "##mask_verをつくりたい\n",
    "###all_data_maskという名前で保存してdataに入れたい\n",
    "##欠損を０、データあるところを１としたもの\n",
    "all_data_mask = []\n",
    "all_data_mask_df = []\n",
    "for x in range(447):\n",
    "    pt_3 = pd.concat([lab50_corr[x], all_data_inj[x], all_data_icd[x]], axis = 1, join = 'outer')\n",
    "    pt_1  = pd.merge(all_data_hw[x],all_data_age[x], on = 'Date', how = 'outer')\n",
    "    pt_2 = pd.merge(pt_1,all_data_vital[x], on = 'Date', how = 'outer')\n",
    "    \n",
    "    pt_3['Date'] = pt_3.index\n",
    "    pt_3['Date'] = pd.to_datetime(pt_3['Date'])\n",
    "\n",
    "    pt_  = pd.merge(pt_2, pt_3, on = 'Date', how = 'outer')\n",
    "\n",
    "    ##性別追加\n",
    "    pt_[\"estim_bln_Male\"] = list_sex_opz[x]\n",
    "    pt_ = pt_.sort_values(['Date'])\n",
    "\n",
    "       ##すべての項目追加\n",
    "    df_list_else = pd.DataFrame(columns = list_else)\n",
    "    pt_ = pd.concat([pt_, df_list_else], axis = 0, join = 'outer')\n",
    "    pt_ = pd.concat([pt_, df_list_empty_columns], axis = 0, join = 'outer')\n",
    "\n",
    "    ##タイムポイントを300日に設定しているので300より多い場合は早い日付けをカットする\n",
    "    if len(pt_) > 307:\n",
    "        pt_ = pt_.tail(307)\n",
    "    else:\n",
    "        pt_ = pt_\n",
    "        \n",
    "    #タイムポイント（仮）を振る\n",
    "    xxx = len(pt_)\n",
    "    lis_xxx = list(range(1, xxx+1))\n",
    "    pt_[\"time_point_number\"] = lis_xxx\n",
    "    #タイムポイントを振った患者dfとタイムポイントのdfとタイムポイントをkeyに結合\n",
    "    pt_ = pd.merge(pt_, df_empty_timepoint, on = 'time_point_number', how = 'outer')\n",
    "  \n",
    "    #timepointを消す\n",
    "    pt_ = pt_.drop(\"time_point_number\", axis = 1)\n",
    "    \n",
    "    \n",
    "    #～～～欠損値処理～～～\n",
    "    #age欠損は１つ前で埋める\n",
    "    pt_['estim_Age_groupnum'] = pt_['estim_Age_groupnum'].fillna(method = 'ffill')\n",
    "    #最初のデータより前はないから1番小さい値で埋めるで良きかな\n",
    "    pt_age_min = pt_['estim_Age_groupnum'].min()\n",
    "    pt_['estim_Age_groupnum'] = pt_['estim_Age_groupnum'].fillna(pt_age_min)\n",
    "    \n",
    "    #injの欠損値を０で埋める。注射なしは０で欠損でない。\n",
    "    for i in list_inj7:\n",
    "        pt_[str(i)] = pt_[str(i)].fillna(0)\n",
    "        #injを一応floatへ\n",
    "        pt_[str(i)] = pt_[str(i)].astype('float')\n",
    "    #nameのは1までを０、1からは全部１で埋める\n",
    "    for n in list_icd:\n",
    "        pt_[str(n)] = pt_[str(n)].fillna(method = 'ffill')\n",
    "        pt_[str(n)] = pt_[str(n)].fillna(0)\n",
    "    #nameを一応floatへ\n",
    "        pt_[str(n)] = pt_[str(n)].astype('float')\n",
    "    #labをfloatへ\n",
    "    for j in list_lab_head_corr:\n",
    "        pt_[str(j)] = pt_[str(j)].astype('float')\n",
    "        #欠損値を平均で埋め、ない。\n",
    "        #完全な欠損（1回もその検査を行っていない）は問題なくて検査しなかったのかなという判断で０埋め、ない。\n",
    "    ##身長体重objectからfloatへ\n",
    "    pt_['身長'] = pt_['身長'].astype(float)\n",
    "    pt_['身長'] = pt_['身長'].fillna(pt_['身長'].mean())\n",
    "    \n",
    "    pt_['体重'] = pt_['体重'].astype(float)\n",
    "    ##欠損値を平均で埋め、ない。\n",
    "    \n",
    "    ##Dateの1個前のDateとの差Δtを求める\n",
    "    #Δt求める\n",
    "    pt_['Δt'] = (pt_['Date'].diff()).dt.days\n",
    "    pt_['Δt'] = pt_['Δt'].astype('float')\n",
    "    ##Date欠損どうしよう～～～とりあえず０で埋め、ない。\n",
    "    pt_ = pt_.drop(columns = 'Date')\n",
    "\n",
    "    ##欠損以外を１で埋める\n",
    "        #pt_[pt_.notnull()] = 1\n",
    "    ##欠損を０で埋める\n",
    "        #pt_.fillna(0, inplace = True) \n",
    "        \n",
    "    ##こっちのほうが良さげ\n",
    "    pt_isnull = pt_.isnull()\n",
    "    pt_mask = pt_isnull.replace({True : 0, False : 1})\n",
    "    \n",
    "    pt_mask = pd.concat([df_list_empty_columns, pt_mask], axis = 0, join = 'outer')\n",
    "    for j in list_empty_columns:\n",
    "        pt_mask[str(i)] = pt_mask[str(i)].astype('float')\n",
    "    pt_mask['Δt'] = pt_mask['Δt'].astype('float')\n",
    "    pt_mask['estim_bln_Male'] = pt_mask['estim_bln_Male'].astype('float')\n",
    "    pt_mask = pt_mask.astype('float')\n",
    "##numpy 3次元へ変換\n",
    "    all_data_mask.append(pt_mask.values)\n",
    "    all_data_mask_df.append(pt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#欠損率を見たい\n",
    "all_data_mask_df[0]\n",
    "\n",
    "#有効なタイムステップでカット\n",
    "all_data_mask_df_ = []\n",
    "for i in range(447):\n",
    "    x_dim2_one = all_data_mask_df[i]\n",
    "    x_dim2_one = pd.DataFrame(x_dim2_one)\n",
    "    all_data_steps_one = all_data_steps[i]\n",
    "    x_dim2_one = x_dim2_one.head(all_data_steps_one)\n",
    "    x_dim2_one['pt_no'] = i\n",
    "    all_data_mask_df_.append(x_dim2_one)\n",
    "all_data_mask_df_[0]\n",
    "all_data_mask_df_merge = pd.concat(all_data_mask_df_)\n",
    "all_data_mask_df_merge\n",
    "\n",
    "all_data_mask_df_merge['time_steps'] = all_data_mask_df_merge.groupby('pt_no').cumcount()\n",
    "\n",
    "all_data_mask_df_merge = pd.DataFrame(all_data_mask_df_merge).set_index(['pt_no', 'time_steps'])\n",
    "\n",
    "\n",
    "#not_injname_kakuninn_mask_merge = pd.DataFrame(not_injname_kakuninn_mask_merge)\n",
    "all_data_mask_df_merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#項目ごとの欠損率\n",
    "#項目ごとの欠損率\n",
    "all_mask_nan = (all_data_mask_df_merge == 0).sum()#欠損あり数出してる\n",
    "all_mask_nan = pd.DataFrame(all_mask_nan)\n",
    "all_mask_nan = all_mask_nan.T\n",
    "all_mask_nan_rate = all_mask_nan *100/73998##～欠損率です～\n",
    "all_mask_nan\n",
    "\n",
    "#全体の欠損率\n",
    "all_mask_nan_all = all_mask_nan.sum(axis = 1)\n",
    "all_mask_nan_allrate = all_mask_nan_all*100/(81*73998)\n",
    "\n",
    "all_mask_nan_allrate###53.898734%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 108)\n",
    "all_data_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df[0].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stepsも読み込み\n",
    "import numpy\n",
    "all_data_steps = numpy.load('pt_id/data/all_data_steps.npy')\n",
    "all_data_steps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##カラム減らしたバージョン\n",
    "#maskはall_dataと一緒ですたぶん"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データセット分けたり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#injとそれ以外のdf分けて作る\n",
    "all_data_inj_df = []\n",
    "inj_kakuninn=[]\n",
    "all_data_not_inj = []\n",
    "not_inj_kakuninn = []\n",
    "for x in range(447):\n",
    "    all_data_inj_one = all_data_df[x].filter(items=list_inj7, axis='columns')\n",
    "    all_data_inj_df.append(all_data_inj_one.values)\n",
    "    inj_kakuninn.append(all_data_inj_one)\n",
    "    list_t = ['Δt']\n",
    "    list_not_inj = list_else + list_lab_head_corr + list_icd + list_t\n",
    "    all_data_not_inj_one = all_data_df[x].filter(items=list_not_inj, axis='columns')\n",
    "    all_data_not_inj.append(all_data_not_inj_one.values)\n",
    "    not_inj_kakuninn.append(all_data_not_inj_one)\n",
    "print(all_data_inj_df[0].shape)\n",
    "print(all_data_not_inj[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#injとそれ以外のdf分けて作る\n",
    "#maskのも作る\n",
    "#injnameとそれ以外のdf分けて作る\n",
    "#maskのも作る\n",
    "all_data_inj_mask_df = []\n",
    "inj_kakuninn_mask = []\n",
    "all_data_not_inj_mask = []\n",
    "not_inj_kakuninn_mask = []\n",
    "for i in range(447):\n",
    "    all_data_inj_one = all_data_mask_df[i].filter(items = list_inj7, axis = 'columns')\n",
    "    all_data_inj_mask_df.append(all_data_inj_one.values)\n",
    "    inj_kakuninn_mask.append(all_data_inj_one)\n",
    "    all_data_not_inj_one = all_data_mask_df[i].filter(items = list_not_inj, axis = 'columns')\n",
    "    all_data_not_inj_mask.append(all_data_not_inj_one.values)\n",
    "    not_inj_kakuninn_mask.append(all_data_not_inj_one)\n",
    "#all_dataと同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#欠損率を見たい\n",
    "not_inj_kakuninn_mask[0]\n",
    "\n",
    "#有効なタイムステップでカット\n",
    "not_inj_kakuninn_mask_ = []\n",
    "for i in range(447):\n",
    "    x_dim2_one = not_inj_kakuninn_mask[i]\n",
    "    x_dim2_one = pd.DataFrame(x_dim2_one)\n",
    "    all_data_steps_one = all_data_steps[i]\n",
    "    x_dim2_one = x_dim2_one.head(all_data_steps_one)\n",
    "    x_dim2_one['pt_no'] = i\n",
    "    not_inj_kakuninn_mask_.append(x_dim2_one)\n",
    "not_inj_kakuninn_mask_[0]\n",
    "not_inj_kakuninn_mask_merge = pd.concat(not_inj_kakuninn_mask_)\n",
    "not_inj_kakuninn_mask_merge\n",
    "\n",
    "not_inj_kakuninn_mask_merge['time_steps'] = not_inj_kakuninn_mask_merge.groupby('pt_no').cumcount()\n",
    "\n",
    "not_inj_kakuninn_mask_merge = pd.DataFrame(not_inj_kakuninn_mask_merge).set_index(['pt_no', 'time_steps'])\n",
    "\n",
    "\n",
    "#not_injname_kakuninn_mask_merge = pd.DataFrame(not_injname_kakuninn_mask_merge)\n",
    "not_inj_kakuninn_mask_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#項目ごとの欠損率\n",
    "not_inj_nan = (not_inj_kakuninn_mask_merge == 0).sum()#欠損あり数出してる\n",
    "not_inj_nan = pd.DataFrame(not_inj_nan)\n",
    "not_inj_nan = not_inj_nan.T\n",
    "not_inj_nan_rate = not_inj_nan *100/73998##～欠損率です～\n",
    "not_inj_nan\n",
    "\n",
    "#全体の欠損率\n",
    "not_inj_nan_all = not_inj_nan.sum(axis = 1)\n",
    "not_inj_nan_allrate = not_inj_nan_all*100/(67*73998)\n",
    "\n",
    "not_inj_nan_allrate###53.898734%\n",
    "not_inj_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestepも同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#injnameとそれ以外のdf分けて作る\n",
    "all_data_injname_df = []\n",
    "injname_kakuninn = []\n",
    "all_data_not_injname = []\n",
    "not_injname_kakuninn = []\n",
    "list_injname = list_inj7 + list_icd\n",
    "list_not_injname = list_else + list_lab_head_corr + list_t\n",
    "for i in range(447):\n",
    "    all_data_injname_one = all_data_df[i].filter(items = list_injname, axis = 'columns')\n",
    "    all_data_injname_df.append(all_data_injname_one.values)\n",
    "    injname_kakuninn.append(all_data_injname_one)\n",
    "    all_data_not_injname_one = all_data_df[i].filter(items = list_not_injname, axis = 'columns')\n",
    "    all_data_not_injname.append(all_data_not_injname_one.values)\n",
    "    not_injname_kakuninn.append(all_data_not_injname_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#injnameとそれ以外のdf分けて作る\n",
    "#maskのも作る\n",
    "all_data_injname_mask_df = []\n",
    "injname_kakuninn_mask = []\n",
    "all_data_not_injname_mask = []\n",
    "not_injname_kakuninn_mask = []\n",
    "for i in range(447):\n",
    "    all_data_injname_one = all_data_mask_df[i].filter(items = list_injname, axis = 'columns')\n",
    "    all_data_injname_mask_df.append(all_data_injname_one.values)\n",
    "    injname_kakuninn_mask.append(all_data_injname_one)\n",
    "    all_data_not_injname_one = all_data_mask_df[i].filter(items = list_not_injname, axis = 'columns')\n",
    "    all_data_not_injname_mask.append(all_data_not_injname_one.values)\n",
    "    not_injname_kakuninn_mask.append(all_data_not_injname_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_injname_kakuninn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#欠損率を見たい\n",
    "not_injname_kakuninn_mask[0]\n",
    "\n",
    "#有効なタイムステップでカット\n",
    "not_injname_kakuninn_mask_ = []\n",
    "for i in range(447):\n",
    "    x_dim2_one = not_injname_kakuninn_mask[i]\n",
    "    x_dim2_one = pd.DataFrame(x_dim2_one)\n",
    "    all_data_steps_one = all_data_steps[i]\n",
    "    x_dim2_one = x_dim2_one.head(all_data_steps_one)\n",
    "    x_dim2_one['pt_no'] = i\n",
    "    not_injname_kakuninn_mask_.append(x_dim2_one)\n",
    "not_injname_kakuninn_mask_[0]\n",
    "not_injname_kakuninn_mask_merge = pd.concat(not_injname_kakuninn_mask_)\n",
    "not_injname_kakuninn_mask_merge\n",
    "\n",
    "not_injname_kakuninn_mask_merge['time_steps'] = not_injname_kakuninn_mask_merge.groupby('pt_no').cumcount()\n",
    "\n",
    "not_injname_kakuninn_mask_merge = pd.DataFrame(not_injname_kakuninn_mask_merge).set_index(['pt_no', 'time_steps'])\n",
    "\n",
    "\n",
    "#not_injname_kakuninn_mask_merge = pd.DataFrame(not_injname_kakuninn_mask_merge)\n",
    "not_injname_kakuninn_mask_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_notinjname_columns = not_injname_kakuninn[0].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_notinjname_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 60)\n",
    "not_injname_kakuninn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#項目ごとの欠損率\n",
    "not_injname_nan = (not_injname_kakuninn_mask_merge == 0).sum()#欠損あり数出してる\n",
    "not_injname_nan = pd.DataFrame(not_injname_nan)\n",
    "not_injname_nan = not_injname_nan.T\n",
    "not_injname_nan_rate = not_injname_nan *100/73998##～欠損率です～\n",
    "not_injname_nan\n",
    "\n",
    "#全体の欠損率\n",
    "not_injname_nan_all = not_injname_nan.sum(axis = 1)\n",
    "not_injname_nan_allrate = not_injname_nan_all*100/(59*73998)\n",
    "\n",
    "not_injname_nan_allrate###61.207037%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timestepは同じ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deathもきれいにしたい\n",
    "df_PT_ID = pd.DataFrame()\n",
    "df_PT_ID['PT_ID'] = list_PT_ID\n",
    "all_data_death = pd.merge(df_PT_ID, df_death_opz, on = 'PT_ID', how = 'outer')\n",
    "print(all_data_death)\n",
    "#死亡の有無のところ、NaNに０、＋に１を入れる\n",
    "death_isnull = all_data_death.isnull()\n",
    "all_data_death = death_isnull.replace({True : 0, False : 1})\n",
    "all_data_death['PT_no'] = all_data_death.index\n",
    "all_data_death.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_pt = all_data_death[all_data_death['死亡の有無'] == 1]\n",
    "death_pt = list(death_pt['PT_no'])\n",
    "not_death_pt = all_data_death[all_data_death['死亡の有無'] == 0]\n",
    "not_death_pt = list(not_death_pt['PT_no'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_notinjname_ver = []\n",
    "s_notinjname_ver = []\n",
    "d_notinjname_ver_mask = []\n",
    "s_notinjname_ver_mask = []\n",
    "\n",
    "d_notinj_ver = []\n",
    "s_notinj_ver = []\n",
    "d_notinj_ver_mask = []\n",
    "s_notinj_ver_mask = []\n",
    "\n",
    "d_ver = []\n",
    "s_ver = []\n",
    "d_ver_mask = []\n",
    "s_ver_mask = []\n",
    "d_ver_timestep = []\n",
    "s_ver_timestep = []\n",
    "for i in death_pt:\n",
    "    notinjname_ver_one = not_injname_kakuninn[i]\n",
    "    notinjname_ver_mask_one = not_injname_kakuninn_mask[i]\n",
    "    d_notinjname_ver.append(notinjname_ver_one.values)\n",
    "    d_notinjname_ver_mask.append(notinjname_ver_mask_one.values)\n",
    "    \n",
    "    d_notinj_ver_one = not_inj_kakuninn[i]\n",
    "    d_notinj_ver_mask_one = not_inj_kakuninn_mask[i]\n",
    "    d_notinj_ver.append(d_notinj_ver_one.values)\n",
    "    d_notinj_ver_mask.append(d_notinj_ver_mask_one.values)\n",
    "    \n",
    "    d_ver_one = all_data_df[i]\n",
    "    d_ver_mask_one = all_data_mask_df[i]\n",
    "    d_ver_timestep_one = all_data_steps[i]\n",
    "    d_ver.append(d_ver_one.values)\n",
    "    d_ver_mask.append(d_ver_mask_one.values)\n",
    "    d_ver_timestep.append(d_ver_timestep_one)\n",
    "for i in not_death_pt:\n",
    "    notinjname_ver_one = not_injname_kakuninn[i]\n",
    "    notinjname_ver_mask_one = not_injname_kakuninn_mask[i]\n",
    "    s_notinjname_ver.append(notinjname_ver_one.values)\n",
    "    s_notinjname_ver_mask.append(notinjname_ver_mask_one.values)\n",
    "    \n",
    "    s_notinj_ver_one = not_inj_kakuninn[i]\n",
    "    s_notinj_ver_mask_one = not_inj_kakuninn_mask[i]\n",
    "    s_notinj_ver.append(s_notinj_ver_one.values)\n",
    "    s_notinj_ver_mask.append(s_notinj_ver_mask_one.values)\n",
    "    \n",
    "    s_ver_one = all_data_df[i]\n",
    "    s_ver_mask_one = all_data_mask_df[i]\n",
    "    s_ver_timestep_one = all_data_steps[i]\n",
    "    s_ver.append(s_ver_one.values)\n",
    "    s_ver_mask.append(s_ver_mask_one.values)\n",
    "    s_ver_timestep.append(s_ver_timestep_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存等々"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存\n",
    "#all_data_ver.npy\n",
    "#all_inj_ver all_not_inj_ver\n",
    "\n",
    "#リストからなんぱいへ変換\n",
    "all_data = np.array(all_data)\n",
    "all_data_inj_df = np.array(all_data_inj_df)\n",
    "all_data_not_inj = np.array(all_data_not_inj)\n",
    "\n",
    "all_data_injname_df = np.array(all_data_injname_df)\n",
    "all_data_not_injname = np.array(all_data_not_injname)\n",
    "all_data_injname_mask_df = np.array(all_data_injname_mask_df)\n",
    "all_data_not_injname_mask = np.array(all_data_not_injname_mask)\n",
    "\n",
    "#保存\n",
    "np.save('pt_id/all_data_ver', all_data)\n",
    "np.save('pt_id/all_inj_ver', all_data_inj_df)\n",
    "np.save('pt_id/all_not_inj_ver', all_data_not_inj)\n",
    "\n",
    "np.save('pt_id/all_injname_ver', all_data_injname_df)\n",
    "np.save('pt_id/data/1215/all_not_injname_ver', all_data_not_injname)\n",
    "np.save('pt_id/all_injname_ver_mask', all_data_injname_mask_df)\n",
    "np.save('pt_id/data/1215/all_not_injname_ver_mask', all_data_not_injname_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#リストからなんぱいへ変換\n",
    "d_notinjname_ver = np.array(d_notinjname_ver)\n",
    "s_notinjname_ver = np.array(s_notinjname_ver)\n",
    "d_notinjname_ver_mask = np.array(d_notinjname_ver_mask)\n",
    "s_notinjname_ver_mask = np.array(s_notinjname_ver_mask)\n",
    "\n",
    "d_notinj_ver = np.array(d_notinj_ver)\n",
    "s_notinj_ver = np.array(s_notinj_ver)\n",
    "d_notinj_ver_mask = np.array(d_notinj_ver_mask)\n",
    "s_notinj_ver_mask = np.array(s_notinj_ver_mask)\n",
    "\n",
    "d_ver = np.array(d_ver)\n",
    "s_ver = np.array(s_ver)\n",
    "d_ver_mask = np.array(d_ver_mask)\n",
    "s_ver_mask = np.array(s_ver_mask)\n",
    "d_ver_timestep = np.array(d_ver_timestep)\n",
    "s_ver_timestep = np.array(s_ver_timestep)\n",
    "\n",
    "#保存\n",
    "np.save('pt_id/data/1215/d_notinjname_ver', d_notinjname_ver)\n",
    "np.save('pt_id/data/1215/s_notinjname_ver', s_notinjname_ver)\n",
    "np.save('pt_id/data/1215/d_notinjname_ver_mask', d_notinjname_ver_mask)\n",
    "np.save('pt_id/data/1215/s_notinjname_ver_mask', s_notinjname_ver_mask)\n",
    "np.save('pt_id/data/1215/d_notinj_ver', d_notinj_ver)\n",
    "np.save('pt_id/data/1215/s_notinj_ver', s_notinj_ver)\n",
    "np.save('pt_id/data/1215/d_notinj_ver_mask', d_notinj_ver_mask)\n",
    "np.save('pt_id/data/1215/s_notinj_ver_mask', s_notinj_ver_mask)\n",
    "np.save('pt_id/data/1215/d_ver', d_ver)\n",
    "np.save('pt_id/data/1215/s_ver', s_ver)\n",
    "np.save('pt_id/data/1215/d_ver_mask', d_ver_mask)\n",
    "np.save('pt_id/data/1215/s_ver_mask', s_ver_mask)\n",
    "np.save('pt_id/data/1215/d_ver_timestep',d_ver_timestep)\n",
    "np.save('pt_id/data/1215/s_ver_timestep', s_ver_timestep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストjblよみこみ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stepsも読み込み\n",
    "import numpy\n",
    "import joblib\n",
    "all_data_steps = numpy.load('pt_id/data/all_data_steps.npy')\n",
    "all_data_steps[0]\n",
    "\n",
    "#dim\n",
    "#,lr=0.005\n",
    "#ttt\n",
    "#の潜在状態空間の点（ヒートマップのデータ）を読み込み\n",
    "#[\"z_params\"][0]=>平均：データ数　x タイムステップ x 状態空間次元\n",
    "obj_dim2=joblib.load(\"pt_id/model/result_save_ds/notinjname_dim16/result_notinjname_dim16_lr01_FFF/test.jbl\")\n",
    "x_dim2 = obj_dim2[\"z_params\"][0]\n",
    "\n",
    "df_x_dim2 = []\n",
    "for i in range(447):\n",
    "    x_dim2_one = x_dim2[i]\n",
    "    x_dim2_one = pd.DataFrame(x_dim2_one)\n",
    "    all_data_steps_one = all_data_steps[i]\n",
    "    x_dim2_one = x_dim2_one.head(all_data_steps_one)\n",
    "    x_dim2_one['pt_no'] = i\n",
    "    df_x_dim2.append(x_dim2_one)\n",
    "print(df_x_dim2[0])\n",
    "\n",
    "concat_df_x_dim2 = pd.concat(df_x_dim2)\n",
    "concat_df_x_dim2 = concat_df_x_dim2.reset_index(drop=True)\n",
    "\n",
    "list_pt_no = concat_df_x_dim2['pt_no']\n",
    "df_pt_no = pd.DataFrame(list_pt_no)\n",
    "\n",
    "#保存\n",
    "df_pt_no.to_csv('pt_id/df_pt_no', index = False)\n",
    "\n",
    "concat_df_x_dim2 = concat_df_x_dim2.drop(['pt_no'], axis = 1)\n",
    "\n",
    "\n",
    "concat_df_x_dim2\n",
    "\n",
    "#主成分分析で2次元にしてみる\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca.fit(concat_df_x_dim2)\n",
    "pca_x_dim2 = pca.transform(concat_df_x_dim2)\n",
    "\n",
    "pca_x_dim2 = pd.DataFrame(pca_x_dim2)\n",
    "\n",
    "#解体作業\n",
    "pca_x_dim2 = pd.concat([pca_x_dim2, df_pt_no], axis = 1, join = 'inner')\n",
    "pca_x_dim2\n",
    "pca_dim2 = []\n",
    "for x in range(447):\n",
    "    pca_i_dim2 = pca_x_dim2[pca_x_dim2['pt_no'] == x]\n",
    "    pca_i_dim2 = pca_i_dim2.drop(['pt_no'], axis = 1)\n",
    "    pca_dim2.append(pca_i_dim2)\n",
    "pca_dim2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df_x_dim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df_x_dim2.to_csv('concat_df_pca_dkf16pca.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dim2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCAでなくTSNEで次元削減してみる\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components = 2, perplexity = 30, random_state = 0)#perplexityの設定が大事らしい\n",
    "vecs_list = tsne.fit_transform(concat_df_x_dim2)\n",
    "vecs_list = pd.DataFrame(vecs_list)\n",
    "vecs_list\n",
    "#解体作業\n",
    "tsne_x_dim2 = pd.concat([vecs_list, df_pt_no], axis = 1, join = 'inner')\n",
    "tsne_x_dim2\n",
    "tsne_dim2 = []\n",
    "for x in range(447):\n",
    "    tsne_i_dim2 = tsne_x_dim2[tsne_x_dim2['pt_no'] == x]\n",
    "    tsne_i_dim2 = tsne_i_dim2.drop(['pt_no'], axis = 1)\n",
    "    tsne_dim2.append(tsne_i_dim2)\n",
    "tsne_dim2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_list.to_csv('concat_df_dkf16tsne2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tsne_dim2[1][0])\n",
    "plt.plot(tsne_dim2[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "クラスタリングの解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df_merge.to_csv('all_data_df_merge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_dim2_merge_cluster = list(tsne_dim2_merge['clusterABC'])\n",
    "tsne_dim2_merge_cluster\n",
    "all_data_df_merge['clusterABC'] = tsne_dim2_merge_cluster\n",
    "all_data_df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_clu_one = []\n",
    "for x in range(447):\n",
    "    all_data_df_merge_one = all_data_df_merge.query('pt_no == @x')\n",
    "    all_data_clu_one.append(all_data_df_merge_one)\n",
    "all_data_clu_one[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#遷移定量化\n",
    "clu_replace = []\n",
    "for i in range(447):\n",
    "    clu_replace_one = all_data_clu_one[i]\n",
    "    clu_replace_one = clu_replace_one['clusterABC']\n",
    "    clu_replace_one = clu_replace_one.replace({'clusterD':0, 'clusterC':0, 'clusterB':5, 'clusterA':3, 'cluster-1':'nan'})\n",
    "    clu_replace_one = clu_replace_one.fillna(method = 'ffill')\n",
    "    clu_replace_one = clu_replace_one.astype('float')\n",
    "    clu_replace_one = clu_replace_one.diff()\n",
    "    \n",
    "    clu_replace_one = clu_replace_one.value_counts()\n",
    "    clu_replace_one = pd.DataFrame(clu_replace_one)\n",
    "    clu_replace_one = clu_replace_one.rename(columns = {'clusterABC': i})\n",
    "    clu_replace.append(clu_replace_one)\n",
    "    \n",
    "clu_replace[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "death_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clu_replace_all = pd.concat(clu_replace,axis = 1)\n",
    "clu_replace_all = clu_replace_all.T\n",
    "clu_replace_all.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_cluA = all_data_df_merge[all_data_df_merge['clusterABC'] == 'clusterA'] \n",
    "all_data_cluB = all_data_df_merge[all_data_df_merge['clusterABC'] == 'clusterB']\n",
    "all_data_cluC = all_data_df_merge[all_data_df_merge['clusterABC'] == 'clusterC'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_cluA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#基本統計量出してみるぞい\n",
    "#all_data_cluA、all_data_cluB、all_data_cluC\n",
    "avgs_A = all_data_cluA.describe()\n",
    "avgs_B = all_data_cluB.describe()\n",
    "avgs_C = all_data_cluC.describe()\n",
    "avgs_A#死亡左"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs_B#しぼう右"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs_C#何もないとこのこ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "a_columns = all_data_cluA.columns\n",
    "for i in a_columns:\n",
    "    #plt.boxplot(all_data_cluA[str(i)], vert=False)\n",
    "    df_a = all_data_cluA[str(i)]    \n",
    "    df_b = all_data_cluB[str(i)]\n",
    "    df_c = all_data_cluC[str(i)]\n",
    "    data = (df_a, df_b, df_c)\n",
    "    figq, ax1 = plt.subplots()\n",
    "    ax1.set_xticklabels(['A', 'B', 'C'])\n",
    "    ax1.set_title(str(i))\n",
    "    ax1.boxplot(data, showmeans=True)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_AAA = pd.concat([all_data_cluA['0201000'], all_data_cluA['0203000'], all_data_cluA['0202000'], all_data_cluA['0421000'], all_data_cluA['0206010'], all_data_cluA['0206040']],axis =1)\n",
    "all_BBB = pd.concat([all_data_cluB['0201000'], all_data_cluB['0203000'], all_data_cluB['0202000'], all_data_cluB['0421000'], all_data_cluB['0206010'], all_data_cluB['0206040']],axis =1)\n",
    "all_CCC = pd.concat([all_data_cluC['0201000'], all_data_cluC['0203000'], all_data_cluC['0202000'], all_data_cluC['0421000'], all_data_cluC['0206010'], all_data_cluC['0206040']],axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_cluA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_AAA_sum_05 = (all_AAA == 0.5).sum()\n",
    "all_AAA_sum_0 = (all_AAA == 0).sum()\n",
    "all_AAA_sum_1 = (all_AAA == 1).sum()\n",
    "\n",
    "all_AAA_sum_0 = pd.DataFrame({'+L': all_AAA_sum_0})\n",
    "all_AAA_sum_05 = pd.DataFrame({'-' : all_AAA_sum_05})\n",
    "all_AAA_sum_1 = pd.DataFrame({'+H': all_AAA_sum_1})\n",
    "\n",
    "\n",
    "all_AAA_sum = pd.concat([all_AAA_sum_05, all_AAA_sum_1, all_AAA_sum_0], axis = 1)\n",
    "all_AAA_sum = all_AAA_sum.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_BBB_sum_05 = (all_BBB == 0.5).sum()\n",
    "all_BBB_sum_0 = (all_BBB == 0).sum()\n",
    "all_BBB_sum_1 = (all_BBB == 1).sum()\n",
    "\n",
    "all_BBB_sum_0 = pd.DataFrame({'+L': all_BBB_sum_0})\n",
    "all_BBB_sum_05 = pd.DataFrame({'-' : all_BBB_sum_05})\n",
    "all_BBB_sum_1 = pd.DataFrame({'+H': all_BBB_sum_1})\n",
    "\n",
    "\n",
    "all_BBB_sum = pd.concat([all_BBB_sum_05, all_BBB_sum_1, all_BBB_sum_0], axis = 1)\n",
    "all_BBB_sum = all_BBB_sum.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_CCC_sum_05 = (all_CCC == 0.5).sum()\n",
    "all_CCC_sum_0 = (all_CCC == 0).sum()\n",
    "all_CCC_sum_1 = (all_CCC == 1).sum()\n",
    "\n",
    "all_CCC_sum_0 = pd.DataFrame({'+L': all_CCC_sum_0})\n",
    "all_CCC_sum_05 = pd.DataFrame({'-' : all_CCC_sum_05})\n",
    "all_CCC_sum_1 = pd.DataFrame({'+H': all_CCC_sum_1})\n",
    "\n",
    "\n",
    "all_CCC_sum = pd.concat([all_CCC_sum_05, all_CCC_sum_1, all_CCC_sum_0], axis = 1)\n",
    "all_CCC_sum = all_CCC_sum.T\n",
    "AAA_columns = all_CCC_sum.columns\n",
    "AAA_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_CCC_sum = all_CCC_sum.reindex(index = ['+L', '-', '+H'])\n",
    "all_AAA_sum = all_AAA_sum.reindex(index = ['+L', '-', '+H'])\n",
    "all_BBB_sum = all_BBB_sum.reindex(index = ['+L', '-', '+H'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_columns = all_data_cluA.columns\n",
    "for x in AAA_columns:\n",
    "    #plt.boxplot(all_data_cluA[str(i)], vert=False)\n",
    "    df_a = pd.DataFrame({'A':all_AAA_sum[str(x)]})    \n",
    "    df_b = pd.DataFrame({'B':all_BBB_sum[str(x)]})\n",
    "    df_c = pd.DataFrame({'C':all_CCC_sum[str(x)]})\n",
    "    dataset = pd.concat([df_a, df_b, df_c], axis = 1)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    for i in range(len(dataset)):\n",
    "        ax.bar(dataset.columns, dataset.iloc[i], bottom=dataset.iloc[:i].sum())\n",
    "    for j in range(len(dataset.columns)):\n",
    "        plt.text(x=j, y=dataset.iloc[:i, j].sum() + (dataset.iloc[i, j] / 2), s=dataset.iloc[i, j], ha='center', va='bottom', fontsize = 20)\n",
    "    #ax.set(xlabel='クラスター', ylabel='counts')\n",
    "    ax.legend(dataset.index)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#colors = plt.get_cmap('Accent')(np.linspace(0, 1, 3))\n",
    "colors = ('cornflowerblue', 'silver', 'magenta')\n",
    "for x in AAA_columns:\n",
    "    #plt.boxplot(all_data_cluA[str(i)], vert=False)\n",
    "    df_a = pd.DataFrame({'A':all_AAA_sum[str(x)]})    \n",
    "    df_b = pd.DataFrame({'B':all_BBB_sum[str(x)]})\n",
    "    df_c = pd.DataFrame({'C':all_CCC_sum[str(x)]})\n",
    "    dataset = pd.concat([df_a, df_b, df_c], axis = 1)\n",
    "    \n",
    "    \n",
    "    plot_dataset = pd.DataFrame(index = dataset.index)\n",
    "    for col in dataset.columns:\n",
    "        plot_dataset[col] = round(100 * dataset[col] / dataset[col].sum(), 1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    for i in range(len(plot_dataset)):\n",
    "        ax.bar(plot_dataset.columns, plot_dataset.iloc[i], bottom=plot_dataset.iloc[:i].sum(), color = colors[i])\n",
    "        for j in range(len(plot_dataset.columns)):\n",
    "            plt.text(x=j, y=plot_dataset.iloc[:i, j].sum()+(plot_dataset.iloc[i, j]/2), s=f'{plot_dataset.iloc[i, j]}%', ha='center', va='bottom', fontsize = 35 )\n",
    "    #ax.set(xlabel='クラスター', ylabel='売り上げ')\n",
    "    ax.legend(plot_dataset.index, fontsize = 25)\n",
    "    plt.tick_params(labelsize=35)\n",
    "    #plt.xticks(fontsize=10) #rotation=90\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgs_AC = avgs_A - avgs_C\n",
    "avgs_BC = avgs_B - avgs_C\n",
    "mean_AC = avgs_AC.loc['mean']\n",
    "mean_BC = avgs_BC.loc['mean']\n",
    "mean_AC = pd.DataFrame(mean_AC)\n",
    "mean_AC = mean_AC.sort_values('mean')\n",
    "mean_AC\n",
    "mean_BC = pd.DataFrame(mean_BC)\n",
    "mean_BC = mean_BC.sort_values('mean')\n",
    "mean_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_injname_kakuninn[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_injname_kakuninn\n",
    "\n",
    "#PCA\n",
    "df_x_dim2 = []\n",
    "for i in range(447):\n",
    "    x_dim2_one = not_injname_kakuninn[i]\n",
    "    x_dim2_one = pd.DataFrame(x_dim2_one)\n",
    "    all_data_steps_one = all_data_steps[i]\n",
    "    x_dim2_one = x_dim2_one.head(all_data_steps_one)\n",
    "    x_dim2_one['pt_no'] = i\n",
    "    df_x_dim2.append(x_dim2_one)\n",
    "df_x_dim2[0]\n",
    "\n",
    "concat_df_x_dim2 = pd.concat(df_x_dim2)\n",
    "concat_df_x_dim2 = concat_df_x_dim2.reset_index(drop=True)\n",
    "\n",
    "list_pt_no = concat_df_x_dim2['pt_no']\n",
    "df_pt_no = pd.DataFrame(list_pt_no)\n",
    "\n",
    "concat_df_x_dim2 = concat_df_x_dim2.drop(['pt_no'], axis = 1)\n",
    "\n",
    "\n",
    "concat_df_x_dim2\n",
    "\n",
    "#主成分分析で2次元にしてみる\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=16)\n",
    "\n",
    "pca.fit(concat_df_x_dim2)\n",
    "pca_x_dim2 = pca.transform(concat_df_x_dim2)\n",
    "\n",
    "pca_x_dim2 = pd.DataFrame(pca_x_dim2)\n",
    "\n",
    "#解体作業\n",
    "pca_x_dim2 = pd.concat([pca_x_dim2, df_pt_no], axis = 1, join = 'inner')\n",
    "pca_x_dim2\n",
    "pca_dim2 = []\n",
    "for x in range(447):\n",
    "    pca_i_dim2 = pca_x_dim2[pca_x_dim2['pt_no'] == x]\n",
    "    pca_i_dim2 = pca_i_dim2.drop(['pt_no'], axis = 1)\n",
    "    pca_dim2.append(pca_i_dim2)\n",
    "pca_dim2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df_x_dim2.to_csv('concat_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df_x_dim2\n",
    "#hozonn\n",
    "#concat_df_x_dim2.to_csv('pt_id/df_concat_not_injname.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PCA16次元からTSNE2次元で次元削減してみる\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components = 2, perplexity = 30, random_state = 0)#perplexityの設定が大事らしい\n",
    "vecs_list = tsne.fit_transform(pca_x_dim2)\n",
    "vecs_list = pd.DataFrame(vecs_list)\n",
    "vecs_list\n",
    "#解体作業\n",
    "tsne_x_dim2 = pd.concat([vecs_list, df_pt_no], axis = 1, join = 'inner')\n",
    "tsne_x_dim2\n",
    "tsne_dim2 = []\n",
    "for x in range(447):\n",
    "    tsne_i_dim2 = tsne_x_dim2[tsne_x_dim2['pt_no'] == x]\n",
    "    tsne_i_dim2 = tsne_i_dim2.drop(['pt_no'], axis = 1)\n",
    "    tsne_dim2.append(tsne_i_dim2)\n",
    "tsne_dim2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_list.to_csv('concat_df_pca16tsne2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TSNE2次元で次元削減してみる\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components = 2, perplexity = 30, random_state = 0)#perplexityの設定が大事らしい\n",
    "vecs_list = tsne.fit_transform(concat_df_x_dim2)\n",
    "vecs_list = pd.DataFrame(vecs_list)\n",
    "vecs_list\n",
    "#解体作業\n",
    "tsne_x_dim2 = pd.concat([vecs_list, df_pt_no], axis = 1, join = 'inner')\n",
    "tsne_x_dim2\n",
    "tsne_dim2 = []\n",
    "for x in range(447):\n",
    "    tsne_i_dim2 = tsne_x_dim2[tsne_x_dim2['pt_no'] == x]\n",
    "    tsne_i_dim2 = tsne_i_dim2.drop(['pt_no'], axis = 1)\n",
    "    tsne_dim2.append(tsne_i_dim2)\n",
    "tsne_dim2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pt_no.to_csv('df_pt_no.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_list.to_csv('concat_df_tsne2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#患者ごとver\n",
    "\n",
    "#分散出す\n",
    "list_tsne_dim2_mean = []\n",
    "for j in range(447):\n",
    "    tsne_dim2_one = tsne_dim2[j]\n",
    "\n",
    "    #5回ずつの平均出す\n",
    "    len_tsne_dim2 = len(tsne_dim2_one)//5\n",
    "    if len_tsne_dim2 >=5:\n",
    "        list_tsne_dim2_5mean = []\n",
    "        for i in range(len_tsne_dim2):\n",
    "            tsne_dim2_5mean_one = tsne_dim2_one.iloc[i*5:i*5 + 1, :]\n",
    "            tsne_dim2_5mean_one = pd.DataFrame(tsne_dim2_5mean_one.mean()).T\n",
    "            list_tsne_dim2_5mean.append(tsne_dim2_5mean_one)\n",
    "        tsne_dim2_5mean = pd.concat(list_tsne_dim2_5mean)\n",
    "        if len_tsne_dim2*5 == len(tsne_dim2_one):\n",
    "            tsne_dim2_5mean = tsne_dim2_5mean\n",
    "        else:\n",
    "            tsne_dim2_lastmean = pd.DataFrame(tsne_dim2_one.iloc[len_tsne_dim2*5:len(tsne_dim2_one), :].mean()).T\n",
    "            tsne_dim2_5mean = pd.concat([tsne_dim2_5mean, tsne_dim2_lastmean], axis = 0)\n",
    "    else:\n",
    "        tsne_dim2_5mean = pd.DataFrame(tsne_dim2_one.iloc[len_tsne_dim2*5:len(tsne_dim2_one), :].mean()).T\n",
    "    \n",
    "    list_tsne_dim2_mean.append(tsne_dim2_5mean)\n",
    "list_tsne_dim2_mean[446]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_dim2_mean_one_s = []\n",
    "for i in not_death_pt:\n",
    "    tsne_dim2_mean_one = list_tsne_dim2_mean[i]\n",
    "    tsne_dim2_mean_one_s.append(tsne_dim2_mean_one)\n",
    "tsne_dim2_mean_one_not_death = pd.concat(tsne_dim2_mean_one_s)\n",
    "tsne_dim2_mean_one_not_death\n",
    "\n",
    "tsne_dim2_mean_not_death_var = tsne_dim2_mean_one_not_death.var()\n",
    "\n",
    "tsne_dim2_mean_one_d = []\n",
    "for i in death_pt:\n",
    "    tsne_dim2_mean_one = list_tsne_dim2_mean[i]\n",
    "    tsne_dim2_mean_one_d.append(tsne_dim2_mean_one)\n",
    "tsne_dim2_mean_one_death = pd.concat(tsne_dim2_mean_one_d)\n",
    "tsne_dim2_mean_one_death\n",
    "\n",
    "tsne_dim2_mean_death_var = tsne_dim2_mean_one_death.var()\n",
    "\n",
    "tsne_dim2_mean_not_death_var\n",
    "tsne_dim2_mean_death_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全患者さんver\n",
    "\n",
    "#分散出す\n",
    "tsne_dim2_mean = pd.concat(list_tsne_dim2_mean)\n",
    "tsne_dim2_mean_var = tsne_dim2_mean.var()\n",
    "tsne_dim2_mean_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_dim2_mean_not_death_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for p in not_death_pt:\n",
    "    x_dim2 = tsne_dim2[p]\n",
    "    x = x_dim2[0]\n",
    "    y = x_dim2[1]\n",
    "    z_1 = x.tail(1)\n",
    "    z_2 = y.tail(1)\n",
    "\n",
    "    plt.plot(x, y, color = 'steelblue',lw = 0.1)\n",
    "    #plt.scatter(x, y, color = 'b', s = 1)\n",
    "    plt.scatter(z_1, z_2, color = 'steelblue', s = 50)\n",
    "    #plotにお名前つける\n",
    "    #plt.annotate(p, (z_1,z_2))\n",
    "\n",
    "for p in death_pt:\n",
    "    x_dim2 = tsne_dim2[p]\n",
    "    x = x_dim2[0]\n",
    "    y = x_dim2[1]\n",
    "    z_1 = x.tail(1)\n",
    "    z_2 = y.tail(1)\n",
    "\n",
    "    plt.plot(x, y, color = 'indianred',lw = 0.1)\n",
    "    #plt.scatter(x, y, color = 'r', s = 1)\n",
    "    plt.scatter(z_1, z_2, color = 'indianred', s = 50)\n",
    "    #plotにお名前つける\n",
    "    #plt.annotate(p, (z_1,z_2))\n",
    "    plt.axes().set_aspect('equal')\n",
    "plt.xticks(color=\"None\")\n",
    "plt.yticks(color=\"None\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for p in not_death_pt:\n",
    "    x_dim2 = tsne_dim2[p]\n",
    "    x = x_dim2[0]\n",
    "    y = x_dim2[1]\n",
    "    z_1 = x.tail(1)\n",
    "    z_2 = y.tail(1)\n",
    "\n",
    "    #plt.plot(x, y, color = 'b',lw = 0.01)\n",
    "    #plt.scatter(x, y, color = 'b', s = 1)\n",
    "    plt.scatter(z_1, z_2, color = 'steelblue', s = 50)\n",
    "    #plotにお名前つける\n",
    "    #plt.annotate(p, (z_1,z_2))\n",
    "\n",
    "for p in death_pt:\n",
    "    x_dim2 = tsne_dim2[p]\n",
    "    x = x_dim2[0]\n",
    "    y = x_dim2[1]\n",
    "    z_1 = x.tail(1)\n",
    "    z_2 = y.tail(1)\n",
    "\n",
    "    #plt.plot(x, y, color = 'r',lw = 0.01)\n",
    "    #plt.scatter(x, y, color = 'r', s = 1)\n",
    "    plt.scatter(z_1, z_2, color = 'indianred', s = 50)\n",
    "    #plotにお名前つける\n",
    "    #plt.annotate(p, (z_1,z_2))\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.xticks(color=\"None\")\n",
    "plt.yticks(color=\"None\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for p in not_death_pt:\n",
    "    x_dim2 = tsne_dim2[p]\n",
    "    x = x_dim2[0]\n",
    "    y = x_dim2[1]\n",
    "    z_1 = x.tail(1)\n",
    "    z_2 = y.tail(1)\n",
    "\n",
    "    #plt.plot(x, y, color = 'steelblue',lw = 0.01)\n",
    "    plt.scatter(x, y, color = 'steelblue', s = 10, alpha = 0.1)\n",
    "    plt.scatter(z_1, z_2, color = 'b', s = 50)\n",
    "    #plotにお名前つける\n",
    "    #plt.annotate(p, (z_1,z_2))\n",
    "plt.grid(True)\n",
    "#plt.savefig(\"fig_save/notinjname_dim16_lr01_FFF/tsne_ds_plot_b_not_death.png\")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for p in death_pt:\n",
    "    x_dim2 = tsne_dim2[p]\n",
    "    x = x_dim2[0]\n",
    "    y = x_dim2[1]\n",
    "    z_1 = x.tail(1)\n",
    "    z_2 = y.tail(1)\n",
    "\n",
    "    #plt.plot(x, y, color = 'indianred',lw = 0.01)\n",
    "    plt.scatter(x, y, color = 'indianred', s = 10, alpha = 0.1)\n",
    "    plt.scatter(z_1, z_2, color = 'r', s = 50)\n",
    "    #plotにお名前つける\n",
    "    #plt.annotate(p, (z_1,z_2))\n",
    "plt.grid(True)\n",
    "\n",
    "#plt.savefig(\"fig_save/notinjname_dim16_lr01_FFF/tsne_ds_plot_r_death.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_df_x_dim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最初からTSNE2次元で次元削減してみる\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components = 2, perplexity = 30, random_state = 0)#perplexityの設定が大事らしい\n",
    "vecs_list = tsne.fit_transform(concat_df_x_dim2)\n",
    "vecs_list = pd.DataFrame(vecs_list)\n",
    "vecs_list\n",
    "#解体作業\n",
    "tsne_x_dim2 = pd.concat([vecs_list, df_pt_no], axis = 1, join = 'inner')\n",
    "tsne_x_dim2\n",
    "tsne_dim2 = []\n",
    "for x in range(447):\n",
    "    tsne_i_dim2 = tsne_x_dim2[tsne_x_dim2['pt_no'] == x]\n",
    "    tsne_i_dim2 = tsne_i_dim2.drop(['pt_no'], axis = 1)\n",
    "    tsne_dim2.append(tsne_i_dim2)\n",
    "tsne_dim2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#患者ごとver\n",
    "\n",
    "#分散出す\n",
    "list_tsne_dim2_mean = []\n",
    "for j in range(447):\n",
    "    tsne_dim2_one = tsne_dim2[j]\n",
    "\n",
    "    #5回ずつの平均出す\n",
    "    len_tsne_dim2 = len(tsne_dim2_one)//5\n",
    "    if len_tsne_dim2 >=5:\n",
    "        list_tsne_dim2_5mean = []\n",
    "        for i in range(len_tsne_dim2):\n",
    "            tsne_dim2_5mean_one = tsne_dim2_one.iloc[i*5:i*5 + 1, :]\n",
    "            tsne_dim2_5mean_one = pd.DataFrame(tsne_dim2_5mean_one.mean()).T\n",
    "            list_tsne_dim2_5mean.append(tsne_dim2_5mean_one)\n",
    "        tsne_dim2_5mean = pd.concat(list_tsne_dim2_5mean)\n",
    "        if len_tsne_dim2*5 == len(tsne_dim2_one):\n",
    "            tsne_dim2_5mean = tsne_dim2_5mean\n",
    "        else:\n",
    "            tsne_dim2_lastmean = pd.DataFrame(tsne_dim2_one.iloc[len_tsne_dim2*5:len(tsne_dim2_one), :].mean()).T\n",
    "            tsne_dim2_5mean = pd.concat([tsne_dim2_5mean, tsne_dim2_lastmean], axis = 0)\n",
    "    else:\n",
    "        tsne_dim2_5mean = pd.DataFrame(tsne_dim2_one.iloc[len_tsne_dim2*5:len(tsne_dim2_one), :].mean()).T\n",
    "    \n",
    "    list_tsne_dim2_mean.append(tsne_dim2_5mean)\n",
    "list_tsne_dim2_mean[446]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_dim2_mean_one_s = []\n",
    "for i in not_death_pt:\n",
    "    tsne_dim2_mean_one = list_tsne_dim2_mean[i]\n",
    "    tsne_dim2_mean_one_s.append(tsne_dim2_mean_one)\n",
    "tsne_dim2_mean_one_not_death = pd.concat(tsne_dim2_mean_one_s)\n",
    "tsne_dim2_mean_one_not_death\n",
    "\n",
    "tsne_dim2_mean_not_death_var = tsne_dim2_mean_one_not_death.var()\n",
    "\n",
    "tsne_dim2_mean_one_d = []\n",
    "for i in death_pt:\n",
    "    tsne_dim2_mean_one = list_tsne_dim2_mean[i]\n",
    "    tsne_dim2_mean_one_d.append(tsne_dim2_mean_one)\n",
    "tsne_dim2_mean_one_death = pd.concat(tsne_dim2_mean_one_d)\n",
    "tsne_dim2_mean_one_death\n",
    "\n",
    "tsne_dim2_mean_death_var = tsne_dim2_mean_one_death.var()\n",
    "\n",
    "tsne_dim2_mean_not_death_var\n",
    "tsne_dim2_mean_death_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_dim2_mean_not_death_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#全患者さんver\n",
    "\n",
    "#分散出す\n",
    "tsne_dim2_mean = pd.concat(list_tsne_dim2_mean)\n",
    "tsne_dim2_mean_var = tsne_dim2_mean.var()\n",
    "tsne_dim2_mean_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for p in not_death_pt:\n",
    "    x_dim2 = tsne_dim2[p]\n",
    "    x = x_dim2[0]\n",
    "    y = x_dim2[1]\n",
    "    z_1 = x.tail(1)\n",
    "    z_2 = y.tail(1)\n",
    "\n",
    "    plt.plot(x, y, color = 'steelblue',lw = 0.1)\n",
    "    #plt.scatter(x, y, color = 'b', s = 1)\n",
    "    plt.scatter(z_1, z_2, color = 'steelblue', s = 50)\n",
    "    #plotにお名前つける\n",
    "    #plt.annotate(p, (z_1,z_2))\n",
    "\n",
    "for p in death_pt:\n",
    "    x_dim2 = tsne_dim2[p]\n",
    "    x = x_dim2[0]\n",
    "    y = x_dim2[1]\n",
    "    z_1 = x.tail(1)\n",
    "    z_2 = y.tail(1)\n",
    "\n",
    "    plt.plot(x, y, color = 'indianred',lw = 0.1)\n",
    "    #plt.scatter(x, y, color = 'r', s = 1)\n",
    "    plt.scatter(z_1, z_2, color = 'indianred', s = 50)\n",
    "    #plotにお名前つける\n",
    "    #plt.annotate(p, (z_1,z_2))\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.xticks(color=\"None\")\n",
    "plt.yticks(color=\"None\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for p in not_death_pt:\n",
    "    x_dim2 = tsne_dim2[p]\n",
    "    x = x_dim2[0]\n",
    "    y = x_dim2[1]\n",
    "    z_1 = x.tail(1)\n",
    "    z_2 = y.tail(1)\n",
    "\n",
    "    #plt.plot(x, y, color = 'b',lw = 0.01)\n",
    "    #plt.scatter(x, y, color = 'b', s = 1)\n",
    "    plt.scatter(z_1, z_2, color = 'steelblue', s = 50)\n",
    "    #plotにお名前つける\n",
    "    #plt.annotate(p, (z_1,z_2))\n",
    "\n",
    "for p in death_pt:\n",
    "    x_dim2 = tsne_dim2[p]\n",
    "    x = x_dim2[0]\n",
    "    y = x_dim2[1]\n",
    "    z_1 = x.tail(1)\n",
    "    z_2 = y.tail(1)\n",
    "\n",
    "    #plt.plot(x, y, color = 'r',lw = 0.01)\n",
    "    #plt.scatter(x, y, color = 'r', s = 1)\n",
    "    plt.scatter(z_1, z_2, color = 'indianred', s = 50)\n",
    "    #plotにお名前つける\n",
    "    #plt.annotate(p, (z_1,z_2))\n",
    "plt.axes().set_aspect('equal')\n",
    "plt.xticks(color=\"None\")\n",
    "plt.yticks(color=\"None\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "for p in not_death_pt:\n",
    "    x_dim2 = tsne_dim2[p]\n",
    "    x = x_dim2[0]\n",
    "    y = x_dim2[1]\n",
    "    z_1 = x.tail(1)\n",
    "    z_2 = y.tail(1)\n",
    "\n",
    "    #plt.plot(x, y, color = 'steelblue',lw = 0.01)\n",
    "    plt.scatter(x, y, color = 'steelblue', s = 10, alpha = 0.1)\n",
    "    plt.scatter(z_1, z_2, color = 'b', s = 50)\n",
    "    #plotにお名前つける\n",
    "    #plt.annotate(p, (z_1,z_2))\n",
    "plt.grid(True)\n",
    "#plt.savefig(\"fig_save/notinjname_dim16_lr01_FFF/tsne_ds_plot_b_not_death.png\")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for p in death_pt:\n",
    "    x_dim2 = tsne_dim2[p]\n",
    "    x = x_dim2[0]\n",
    "    y = x_dim2[1]\n",
    "    z_1 = x.tail(1)\n",
    "    z_2 = y.tail(1)\n",
    "\n",
    "    #plt.plot(x, y, color = 'indianred',lw = 0.01)\n",
    "    plt.scatter(x, y, color = 'indianred', s = 10, alpha = 0.1)\n",
    "    plt.scatter(z_1, z_2, color = 'r', s = 50)\n",
    "    #plotにお名前つける\n",
    "    #plt.annotate(p, (z_1,z_2))\n",
    "plt.grid(True)\n",
    "\n",
    "#plt.savefig(\"fig_save/notinjname_dim16_lr01_FFF/tsne_ds_plot_r_death.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "#iris_dbscan = DBSCAN(eps=1, min_samples=10)\n",
    "iris_dbscan = DBSCAN(eps=0.03, min_samples=5)\n",
    "iris_dbscan.fit(tsne_x_dim2.values[:,0:2])\n",
    "labels = iris_dbscan.labels_\n",
    "print(labels)\n",
    "lis_labels = list(labels)\n",
    "\n",
    "tsne_x_dim2['cluster_DBSCAN'] = ['cluster' +str(x) for x in labels]\n",
    "tsne_x_dim2\n",
    "c = collections.Counter(tsne_x_dim2['cluster_DBSCAN'])\n",
    "c_key = c.keys()\n",
    "c_key = list(c_key)\n",
    "c_key\n",
    "#tsne_dim2_merge.to_csv('tsne_dim2_merge_DBSCAN', index = True)\n",
    "tsne_x_dim2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=5)\n",
    "nearest_neighbors.fit(pca_x_dim2.values[:,0:2])\n",
    "distances, indices = nearest_neighbors.kneighbors(pca_x_dim2.values[:,0:2])\n",
    "distances = np.sort(distances, axis=0)[:, 1]\n",
    "print(distances)\n",
    "plt.plot(distances)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.set_palette(\"Set2\", 31)\n",
    "#sns.set_palette(\"rainbow_r\",30)\n",
    "sns.pairplot(pca_x_dim2, hue = \"cluster_DBSCAN\",size = 10, hue_order = c_key,diag_kind=\"kde\", aspect = 1.5)# diag_kind=\"kde\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.scatterplot(x=tsne_x_dim2[0], y=tsne_x_dim2[1], hue = \"cluster_DBSCAN\", data = tsne_x_dim2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遊び"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_full = []\n",
    "for i in range(447):\n",
    "    x_dim2_one = all_data_df[i]\n",
    "    x_dim2_one = pd.DataFrame(x_dim2_one)\n",
    "    all_data_steps_one = all_data_steps[i]\n",
    "    x_dim2_one = x_dim2_one.head(all_data_steps_one)\n",
    "    all_data_full.append(x_dim2_one)\n",
    "all_data_full[0].tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail_mean = []\n",
    "for i in range(447):\n",
    "    all_data_tail_one = all_data_full[i].tail(10)\n",
    "    all_data_tail_one = all_data_tail_one.mean()\n",
    "    all_data_tail_one = pd.DataFrame(all_data_tail_one)\n",
    "    all_data_tail_one = all_data_tail_one.rename(columns = {0:i})\n",
    "    all_data_tail_one = all_data_tail_one.iloc[:, :]\n",
    "    tail_mean.append(all_data_tail_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df[443].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx_tconcat = pd.concat([tail_mean[251], tail_mean[81], tail_mean[257], tail_mean[326], tail_mean[301], tail_mean[205], tail_mean[107], tail_mean[51], tail_mean[61], tail_mean[261], tail_mean[18], tail_mean[139]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx_tconcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([tail_mean[251], tail_mean[81]], axis = 1)\n",
    "df_corr = df.corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#時系列感じるデータっぽいのみたい\n",
    "#!pip install px\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "df = all_data_full[0]\n",
    "fig = px.line(df,x='Date', y=df.columns[0:108], title=\"all columns\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deathもきれいにしたい\n",
    "df_PT_ID = pd.DataFrame()\n",
    "df_PT_ID['PT_ID'] = list_PT_ID\n",
    "all_data_death = pd.merge(df_PT_ID, df_death_opz, on = 'PT_ID', how = 'outer')\n",
    "print(all_data_death)\n",
    "#死亡の有無のところ、NaNに０、＋に１を入れる\n",
    "death_isnull = all_data_death.isnull()\n",
    "all_data_death = death_isnull.replace({True : 0, False : 1})\n",
    "all_data_death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_df_ = []\n",
    "for i in range(447):\n",
    "    all_data_steps_one = all_data_steps[i]\n",
    "    all_data_df_one = all_data_df[i].head(all_data_steps_one)\n",
    "    all_data_df_.append(all_data_df_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_data_death = all_data_death['死亡の有無']\n",
    "list_all_data_death = list(list_all_data_death)\n",
    "list_all_data_death[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_all_data = []\n",
    "for i in range(447):\n",
    "    all_data_df_one = all_data_df_[i]\n",
    "\n",
    "    all_data_df_one['Death'] = list_all_data_death[i]\n",
    "    all_data_df_one['PT_ID'] = str(i)\n",
    "    all_data_df_one.reset_index(inplace = True)\n",
    "    all_data_df_one = all_data_df_one.rename(columns = {'index':'Date'})\n",
    "    tfm_all_data.append(all_data_df_one)\n",
    "tfm_all_data[0]\n",
    "tfm_all_data = pd.concat(tfm_all_data)\n",
    "tfm_all_data['Date'] = tfm_all_data.groupby('PT_ID').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_all_data = pd.DataFrame(tfm_all_data).set_index(['PT_ID', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfm_all_data.head(75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns = tfm_all_data.columns.to_list()\n",
    "pdf = tfm_all_data\n",
    "cur_t = range(447)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tdqm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "i = 0\n",
    "\n",
    "lines += ['@problemName eutest']\n",
    "lines += ['@timeStamps false']\n",
    "lines += ['@missing false']\n",
    "lines += ['@univariate false']\n",
    "lines += ['@dimensions ' + str(len(X_columns))]\n",
    "lines += ['@equalLength false']\n",
    "lines += ['@classLabel true 0.0 1.0']\n",
    "lines += ['@data']\n",
    "\n",
    "for cur_PT in tqdm(pdf.reset_index()['PT_ID'].unique()):\n",
    "    cur_PT_df = pdf.loc[[cur_PT], :]\n",
    "    t_max = cur_PT_df.reset_index()['Date'].max()\n",
    "\n",
    "    for cur_t in range(t_max+1):\n",
    "        t_start = 0\n",
    "        t_end = cur_t\n",
    "        cur_df = cur_PT_df.query('@t_start <= Date <= @t_end')\n",
    "\n",
    "        cur_items_str = []\n",
    "        for cur_col in X_columns:\n",
    "            cur_item_str = ','.join(cur_df[cur_col].astype('str').to_list())\n",
    "            cur_items_str.append(cur_item_str)\n",
    "\n",
    "        cur_ln = ':'.join(cur_items_str)\n",
    "\n",
    "        cur_ln += ':'\n",
    "        cur_ln += str(cur_df.loc[(cur_PT, cur_t), 'Death'])\n",
    "\n",
    "        lines.append(cur_ln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = './tes_TRAIN.ts'\n",
    "path = 'tes_TRAIN.ts'\n",
    "with open(path, mode='w') as f:\n",
    "    f.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = './tes_TEST.ts'\n",
    "path = 'tes_TEST.ts'\n",
    "with open(path, mode='w') as f:\n",
    "    f.write('\\n'.join(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
